

<!DOCTYPE html>
<html lang="en" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/notes/info/fluid.png">
  <link rel="icon" href="/notes/info/avatar.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Kevin Lee">
  <meta name="keywords" content="">
  
    <meta name="description" content="Linear AlgebraSpan &amp; Linear Dependence在 Machine Learning 裡面，不管原本的 function 是不是 Linear，我們都會試著用 Linear function 來 Approximate 它。  $\text{span}(A_{:,1}, A_{:,2}, \ldots, A_{:,n})$ 被稱為 column space of">
<meta property="og:type" content="article">
<meta property="og:title" content="Deep Learning">
<meta property="og:url" content="https://933yee.github.io/notes/2025/11/06/deep-learning/index.html">
<meta property="og:site_name" content="933yee&#39;s Notes">
<meta property="og:description" content="Linear AlgebraSpan &amp; Linear Dependence在 Machine Learning 裡面，不管原本的 function 是不是 Linear，我們都會試著用 Linear function 來 Approximate 它。  $\text{span}(A_{:,1}, A_{:,2}, \ldots, A_{:,n})$ 被稱為 column space of">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcT9qejWx2-LxS3zRwd4QcDk7cob7ZWCQJhRoxOjFTb5kmmPzmFDrMYrEEkpc5OwNYNJhPY&usqp=CAU">
<meta property="og:image" content="https://upload.wikimedia.org/wikipedia/commons/8/8d/Lipschitz_continuity.png">
<meta property="article:published_time" content="2025-11-06T04:09:09.000Z">
<meta property="article:modified_time" content="2025-12-03T04:46:57.718Z">
<meta property="article:author" content="Kevin Lee">
<meta property="article:tag" content="deep learning">
<meta property="article:tag" content="ai">
<meta property="article:tag" content="machine learning">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcT9qejWx2-LxS3zRwd4QcDk7cob7ZWCQJhRoxOjFTb5kmmPzmFDrMYrEEkpc5OwNYNJhPY&usqp=CAU">
  
  
  
  <title>Deep Learning - 933yee&#39;s Notes</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/notes/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/notes/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/notes/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"933yee.github.io","root":"/notes/","version":"1.9.7","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/info/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/notes/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/notes/js/utils.js" ></script>
  <script  src="/notes/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 7.1.1"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/notes/">
      <strong>933yee&#39;s Notes</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/notes/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>Home</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/notes/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>Archives</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/notes/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>Categories</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/notes/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>Tags</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/notes/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>About</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/notes/info/default.gif') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.7)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="Deep Learning"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-11-06 12:09" pubdate>
          November 6, 2025 pm
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          11k words
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          90 mins
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">Deep Learning</h1>
            
            
              <div class="markdown-body">
                
                <h2 id="Linear-Algebra"><a href="#Linear-Algebra" class="headerlink" title="Linear Algebra"></a>Linear Algebra</h2><h3 id="Span-Linear-Dependence"><a href="#Span-Linear-Dependence" class="headerlink" title="Span &amp; Linear Dependence"></a>Span &amp; Linear Dependence</h3><p>在 Machine Learning 裡面，不管原本的 function 是不是 Linear，我們都會試著用 Linear function 來 Approximate 它。</p>
<ul>
<li><p>$\text{span}(A_{:,1}, A_{:,2}, \ldots, A_{:,n})$ 被稱為 column space of A，記作 $\text{Col}(A)$</p>
</li>
<li><p>$\text{rank}(A)$ 是 $\text{Col}(A)$ 的維度 (dimension)</p>
</li>
<li><p>給定 $A$ 和 $y$，解方程式 $Ax &#x3D; y$，其中 $A$ 是 $m \times n$ 矩陣，$x$ 是 $n \times 1$ 向量，$y$ 是 $m \times 1$ 向量</p>
<ul>
<li><p><strong>一定至少有一個解</strong></p>
<p>因為 $Ax &#x3D; \Sigma_{i} x_i A_{:,i}$，所以 $Ax$ 只不過是 $A$ 的 column vectors 的 linear combination，只要 $y$ 能寫成 $A$ 的 column vectors 的 linear combination，就一定有解</p>
<p>因此，$\text{span}(A_{:,1}, A_{:,2}, \ldots, A_{:,n}) \ni \mathbb{R}^m$，也代表 $n \geq m$</p>
</li>
<li><p><strong>唯一解</strong></p>
<p>$A$ 最多只能有 $m$ 個 Column，也代表 $n &#x3D; m$，且 $A$ 的 column vectors 必須是線性獨立 linearly independent 的</p>
<p>在這種情況下，$A$ 是 full rank 的，且為 invertible，因此 $x &#x3D; A^{-1}y$ 是唯一解</p>
</li>
</ul>
</li>
</ul>
<h3 id="Norms"><a href="#Norms" class="headerlink" title="Norms"></a>Norms</h3><p>norm 是一個 function ($|\cdot|$)，能夠把 vector 映射到非負實數 (non-negative real number)</p>
<h4 id="L-p-norm"><a href="#L-p-norm" class="headerlink" title="$L^p$ norm:"></a>$L^p$ norm:</h4><p>$$<br>|x|_p &#x3D; \left( \sum_i |x_i|^p \right)^{1&#x2F;p}<br>$$</p>
<ul>
<li><p>$p&#x3D;1$: Manhattan norm</p>
</li>
<li><p>$p&#x3D;2$: Euclidean norm (通常直接寫成 $|x|$)</p>
<p>$$<br> |x| &#x3D; (x^Tx)^{1&#x2F;2} &#x3D; \sqrt{\sum_i x_i^2}<br>$$</p>
</li>
<li><p>$p \to \infty$: Maximum norm</p>
<p>$$<br> |x|_{\infty} &#x3D; \max_i |x_i|<br>$$</p>
</li>
<li><p>$x^Ty &#x3D; |x| |y| \cos \theta$，其中 $\theta$ 是 $x$ 和 $y$ 之間的夾角</p>
<ul>
<li>當 $x \perp y$ 時，$x^Ty &#x3D; 0$，稱為 orthogonal。且如果 $|x| &#x3D; |y| &#x3D; 1$ (unit vectors)，則 $x$ 和 $y$ 是 orthonormal 的</li>
</ul>
</li>
</ul>
<h4 id="Matrix-Norms"><a href="#Matrix-Norms" class="headerlink" title="Matrix Norms"></a>Matrix Norms</h4><ul>
<li><p>Frobenius norm:</p>
<p>$$<br>|A|<em>F &#x3D; \sqrt{\sum</em>{i,j} A_{i,j}^2} &#x3D; \sqrt{\operatorname{tr}(A^\top A)}<br>$$</p>
</li>
<li><p>Orthogonal matrix:</p>
<p>每個 column vector 都是 unit vector，且彼此 orthogonal，通常不會特別去區分 orthogonal matrix 和 orthonormal matrix，都稱為 orthogonal matrix</p>
<p>$$<br> Q^TQ &#x3D; QQ^T &#x3D; I<br>$$</p>
<ul>
<li>也代表 $Q^{-1} &#x3D; Q^T$</li>
</ul>
</li>
</ul>
<h3 id="Eigendecomposition"><a href="#Eigendecomposition" class="headerlink" title="Eigendecomposition"></a>Eigendecomposition</h3><p>Decomposition 可以幫助我們快速了解矩陣的性質</p>
<h4 id="Eigen-vectors-Eigen-values"><a href="#Eigen-vectors-Eigen-values" class="headerlink" title="Eigen vectors &amp; Eigen values"></a>Eigen vectors &amp; Eigen values</h4><ul>
<li><p>給定一個方陣 $A$，如果存在一個非零向量 $v$ 和一個純量 $\lambda$，使得</p>
<p>$$<br> Av &#x3D; \lambda v<br>$$</p>
<p>則稱 $v$ 為 $A$ 的 <strong>eigen vector</strong>，$\lambda$ 為對應的 <strong>eigen value</strong></p>
<p>可以想像成，某個 function A 遇到 eigen vector 時，只會對其進行伸縮 (scaling)，不會改變方向</p>
</li>
<li><p>如果 $v$ 是一個 eigen vector，則 $cv$ (c 為非零常數) 也是 eigen vector，對應的 eigen value 不變，所以一般只會討論 unit eigen vectors</p>
</li>
</ul>
<h4 id="Eigendecomposition-1"><a href="#Eigendecomposition-1" class="headerlink" title="Eigendecomposition"></a>Eigendecomposition</h4><ul>
<li><p>任何 <strong>real symmetric matrix</strong> $A \in \mathbb{R}^{n \times n}$ 都可以被分解成</p>
<p>$$<br>A &#x3D; Q \text{diag}(\lambda) Q^T<br>$$</p>
<ul>
<li>$\lambda \in \mathbb{R}^n$: eigen values of $A$，通常會由大到小排序</li>
<li>$Q &#x3D; [v_1, v_2, \ldots, v_n]$: 由 $A$ 的 eigen vectors 組成的 orthogonal matrix</li>
</ul>
</li>
<li><p>Eigendecomposition 不是唯一的</p>
</li>
</ul>
<p><img src="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcT9qejWx2-LxS3zRwd4QcDk7cob7ZWCQJhRoxOjFTb5kmmPzmFDrMYrEEkpc5OwNYNJhPY&usqp=CAU" srcset="/notes/info/loading.gif" lazyload alt="eigendecomposition"></p>
<p>可以想成把矩陣 A 當作一個線性轉換 (linear transformation)，先把空間旋轉到 eigen vector 的方向 (由 Q 決定)，再沿著各個 eigen vector 方向進行伸縮 (由 $\lambda$ 決定)，最後再把空間旋轉回來 (由 $Q^T$ 決定)</p>
<h4 id="Rayleigh’s-Quotient"><a href="#Rayleigh’s-Quotient" class="headerlink" title="Rayleigh’s Quotient"></a>Rayleigh’s Quotient</h4><p>給定一個 real symmetric matrix $A \in \mathbb{R}^{n \times n}$，對任意非零向量 $x \in \mathbb{R}^n$，定義 Rayleigh’s Quotient 為</p>
<p>$$<br>R(x) &#x3D; \frac{x^T A x}{x^T x}<br>$$</p>
<p>且</p>
<p>$$<br>\lambda_{\min} \leq R(x) \leq \lambda_{\max}<br>$$</p>
<ul>
<li>$\lambda_{\min}$: $A$ 的最小 eigen value</li>
<li>$\lambda_{\max}$: $A$ 的最大 eigen value</li>
<li>當且僅當 $x$ 是對應於 $\lambda_{\min}$ 或 $\lambda_{\max}$ 的 eigen vector 時，等號成立</li>
</ul>
<h4 id="Singularity"><a href="#Singularity" class="headerlink" title="Singularity"></a>Singularity</h4><p>若 $A &#x3D; Q \text{diag}(\lambda) Q^T$，則 $A^{-1} &#x3D; Q \text{diag}(\lambda)^{-1} Q^T$</p>
<p>因為 Diagonal matrix 的反矩陣是把對角線元素取倒數，所以當 $\lambda_i &#x3D; 0$ 時，$A$ 就沒有反矩陣，稱為 singular matrix</p>
<h4 id="Positive-Definiteness"><a href="#Positive-Definiteness" class="headerlink" title="Positive Definiteness"></a>Positive Definiteness</h4><ul>
<li><p>Positive Definite:</p>
<p>對所有非零向量 $x$，都有 $x^T A x &gt; 0$，等價於所有的 eigen values 都是正的</p>
</li>
<li><p>Positive Semi-Definite:</p>
<p>對所有非零向量 $x$，都有 $x^T A x \geq 0$，等價於所有的 eigen values 都是非負的</p>
</li>
</ul>
<p>對於一個 Quadratic Function $f(x) &#x3D; \frac{1}{2} x^T A x - b^T x + c$，可以藉由檢查 $A$ 是否為 <code>Positive Definite</code>、<code>Positive Semi-Definite</code>、<code>Negative Definite</code>、<code>Negative Semi-Definite</code> 來判斷其凸性 (convexity)：</p>
<table>
<thead>
<tr>
<th>Definiteness</th>
<th>Convexity</th>
<th>Condition on Eigenvalues</th>
</tr>
</thead>
<tbody><tr>
<td>Positive Definite</td>
<td>Strictly Convex</td>
<td>All eigenvalues &gt; 0</td>
</tr>
<tr>
<td>Positive Semi-Definite</td>
<td>Convex</td>
<td>All eigenvalues ≥ 0</td>
</tr>
<tr>
<td>Negative Definite</td>
<td>Strictly Concave</td>
<td>All eigenvalues &lt; 0</td>
</tr>
<tr>
<td>Negative Semi-Definite</td>
<td>Concave</td>
<td>All eigenvalues ≤ 0</td>
</tr>
<tr>
<td>Indefinite</td>
<td>Neither</td>
<td>Eigenvalues of mixed signs</td>
</tr>
</tbody></table>
<h3 id="Singular-Value-Decomposition-SVD"><a href="#Singular-Value-Decomposition-SVD" class="headerlink" title="Singular Value Decomposition (SVD)"></a>Singular Value Decomposition (SVD)</h3><p>Eigendecomposition 要求矩陣是方陣 (square matrix) 且為 symmetric matrix，但 SVD 不需要這些限制</p>
<p>任何一個矩陣 $A \in \mathbb{R}^{m \times n}$ 都可以被分解成</p>
<p>$$<br>A &#x3D; U \Sigma V^T<br>$$</p>
<ul>
<li>$U \in \mathbb{R}^{m \times m}$: 左奇異向量 (left singular vectors)，為 $AA^T$ 的 eigen vectors 組成的 orthogonal matrix</li>
<li>$\Sigma \in \mathbb{R}^{m \times n}$: 對角矩陣 (diagonal matrix)，對角線上的元素為 $AA^T$ 或 $A^TA$ 的 eigen values 的平方根，稱為 singular values，且通常會由大到小排序</li>
<li>$V \in \mathbb{R}^{n \times n}$: 右奇異向量 (right singular vectors)，為 $A^TA$ 的 eigen vectors 組成的 orthogonal matrix</li>
</ul>
<h3 id="Moore-Penrose-Pseudoinverse"><a href="#Moore-Penrose-Pseudoinverse" class="headerlink" title="Moore-Penrose Pseudoinverse"></a>Moore-Penrose Pseudoinverse</h3><p>對於一般情況，一個非方陣 $A \in \mathbb{R}^{m \times n}$，我們無法計算其反矩陣 $A^{-1}$，但我們可以用另一個矩陣 $B \in \mathbb{R}^{n \times m}$ ，用來解出 $Ax &#x3D; y$，即 $x &#x3D; By$</p>
<p>Moore-Penrose Pseudoinverse 令 $B &#x3D; A^+$，可以把任務拆成三種 Case：</p>
<ol>
<li><p><strong>$m &#x3D; n$</strong>:</p>
<ul>
<li>有唯一解 (如果 $A$ 是 invertible 的)</li>
<li>解為 $A^+ &#x3D; A^{-1}$</li>
</ul>
</li>
<li><p><strong>$m &lt; n$</strong>:</p>
<ul>
<li>有無限多解</li>
<li>目標是找到一個 $x$，使得 $|x|_2$ 最小，找一個最小範數解</li>
</ul>
</li>
<li><p><strong>$m &gt; n$</strong>:</p>
<ul>
<li>沒有精確解</li>
<li>目標是找到一個 $x$，使得 $|Ax - y|_2$ 最小，找一個最佳近似解</li>
</ul>
</li>
</ol>
<p>Moore-Penrose Pseudoinverse 定義為</p>
<p>$$<br>A^+ &#x3D; \text{lim}_{\lambda \to 0^+} (A^TA + \lambda I)^{-1} A^T<br>$$</p>
<p>其中 $\lambda$ 是一個非常小的正數，用來確保 $A^TA + \lambda I$ 是 Full Rank 的，因此可以被反轉</p>
<p>實際上計算時，可以直接用 SVD 來計算，先把 $A$ 分解成 $A &#x3D; U \Sigma V^T$，則</p>
<p>$$<br>A^+ &#x3D; V \Sigma^+ U^T<br>$$</p>
<p>其中 $\Sigma^+$ 是把 $\Sigma$ 的非零奇異值取倒數後，再轉置得到的矩陣</p>
<h3 id="Traces"><a href="#Traces" class="headerlink" title="Traces"></a>Traces</h3><ul>
<li><p>trace 的定義是矩陣對角線元素的和</p>
<p>$$<br> \text{trace}(A) &#x3D; \sum_{i} A_{i,i}<br>$$</p>
</li>
<li><p>$|A|^2_F &#x3D; \text{trace}(A^TA)$</p>
</li>
<li><p>$\text{trace}(ABC) &#x3D; \text{trace}(BCA) &#x3D; \text{trace}(CAB)$ (cyclic property)</p>
</li>
</ul>
<h3 id="Determinant"><a href="#Determinant" class="headerlink" title="Determinant"></a>Determinant</h3><p>$$<br>\text{det}(A) &#x3D; \sum_i (-1)^{i+1} A_{1,i} \text{det}(A_{-1,-i})<br>$$</p>
<p>其中 $A_{-1,-i}$: 去掉第 1 列和第 i 行後的子矩陣 (submatrix)</p>
<ul>
<li>$\text{det}(A^T) &#x3D; \text{det}(A)$</li>
<li>$\text{det}(AB) &#x3D; \text{det}(A) \text{det}(B)$</li>
<li>$\text{det}(A^{-1}) &#x3D; \frac{1}{\text{det}(A)}$</li>
<li>$\text{det}(A) &#x3D; \prod_i \lambda_i$，其中 $\lambda_i$ 是 $A$ 的 eigen values</li>
</ul>
<p>Determinant 可以被解釋為線性轉換後，空間體積的伸縮比例，也就是 Image of Unit Square (or Cube) 的體積。</p>
<ul>
<li><p>$\text{det}(A) &#x3D; 0$，代表它在某一個維度被壓縮成 0，整個空間就會被壓縮成一個低維度的子空間 (subspace)</p>
</li>
<li><p>$\text{det}(A) &#x3D; 1$，代表形狀可能會被改變，但體積不變</p>
</li>
</ul>
<h2 id="Probability-Information-Theory"><a href="#Probability-Information-Theory" class="headerlink" title="Probability &amp; Information Theory"></a>Probability &amp; Information Theory</h2><h3 id="Random-Variables"><a href="#Random-Variables" class="headerlink" title="Random Variables"></a>Random Variables</h3><ul>
<li><p>Discrete Random Variable: 只能取有限或可數無限個值的隨機變數</p>
</li>
<li><p>Continuous Random Variable: 可以取無限多個值的隨機變數，通常是區間上的所有值</p>
</li>
<li><p>必須有一個對應的 Probability Measure 來描述 Random Variable 的分佈情況</p>
<ul>
<li>Probability Mass Function (PMF)</li>
<li>Probability Density Function (PDF)</li>
</ul>
</li>
</ul>
<h3 id="Probability-Mass-and-Density-Functions"><a href="#Probability-Mass-and-Density-Functions" class="headerlink" title="Probability Mass and Density Functions"></a>Probability Mass and Density Functions</h3><ul>
<li><p>PMF: 定義在離散隨機變數上，描述每個可能取值的機率</p>
<p>$$<br> P(X &#x3D; x_i) &#x3D; p_i<br>$$</p>
<ul>
<li>$\sum_i p_i &#x3D; 1$</li>
</ul>
</li>
<li><p>PDF: 定義在連續隨機變數上，描述在某個區間內取值的機率密度</p>
<p>$$<br> P(a &lt; X &lt; b) &#x3D; \int_{a}^{b} p(x) dx<br>$$</p>
<ul>
<li>$\int_{-\infty}^{\infty} p(x) dx &#x3D; 1$</li>
<li>$p(x) \geq 0$，但 $p(x)$ 本身不代表機率</li>
<li>$p(x) &gt; 1$ 是可能的，只要積分結果不超過 1 即可<ul>
<li>例如，$p(x) &#x3D; 2$ 在區間 $[0, 0.5]$ 上是合法的 PDF，因為 $\int_{0}^{0.5} 2 dx &#x3D; 1$</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Marginal-Probability"><a href="#Marginal-Probability" class="headerlink" title="Marginal Probability"></a>Marginal Probability</h3><p>$$<br>P(X &#x3D; x) &#x3D; \sum_y P(X &#x3D; x, Y &#x3D; y)<br>$$</p>
<p>or</p>
<p>$$<br>P(X &#x3D; x) &#x3D; \int p(X &#x3D; x, Y &#x3D; y) dy<br>$$</p>
<h3 id="Conditional-Probability"><a href="#Conditional-Probability" class="headerlink" title="Conditional Probability"></a>Conditional Probability</h3><p>$$<br>P(X &#x3D; x | Y &#x3D; y) &#x3D; \frac{P(X &#x3D; x, Y &#x3D; y)}{P(Y &#x3D; y)}<br>$$</p>
<h3 id="Indenpendence-Conditional-Independence"><a href="#Indenpendence-Conditional-Independence" class="headerlink" title="Indenpendence &amp; Conditional Independence"></a>Indenpendence &amp; Conditional Independence</h3><ul>
<li><p>Indenpendence:</p>
<p>$X$ 和 $Y$ 是獨立的，當且僅當</p>
<p>$$<br> P(X &#x3D; x, Y &#x3D; y) &#x3D; P(X &#x3D; x) P(Y &#x3D; y)<br>$$</p>
<p>$$<br> P(X &#x3D; x | Y &#x3D; y) &#x3D; P(X &#x3D; x)<br>$$</p>
</li>
<li><p>Conditional Independence:</p>
<p>$X$ 和 $Y$ 在給定 $Z$ 的條件下是獨立的，當且僅當</p>
<p>$$<br> P(X &#x3D; x, Y &#x3D; y | Z &#x3D; z) &#x3D; P(X &#x3D; x | Z &#x3D; z) P(Y &#x3D; y | Z &#x3D; z)<br>$$</p>
<p>$$<br> P(X &#x3D; x | Y &#x3D; y, Z &#x3D; z) &#x3D; P(X &#x3D; x | Z &#x3D; z)<br>$$</p>
</li>
</ul>
<h3 id="Expectation"><a href="#Expectation" class="headerlink" title="Expectation"></a>Expectation</h3><ul>
<li><p>Discrete Random Variable:</p>
<p>$$<br> \mathbb{E}[X] &#x3D; \sum_i x_i P(X &#x3D; x_i)<br>$$</p>
</li>
<li><p>Continuous Random Variable:</p>
<p>$$<br> \mathbb{E}[X] &#x3D; \int_{-\infty}^{\infty} x p(x) dx<br>$$</p>
</li>
<li><p>線性性質:</p>
</li>
</ul>
<p>$$<br>\mathbb{E}[aX + bY] &#x3D; a \mathbb{E}[X] + b \mathbb{E}[Y]<br>$$</p>
<p>$$<br>\mathbb{E}[\mathbb{E}[f(x)]] &#x3D; \mathbb{E}[f(x)]<br>$$</p>
<p>$$<br>\mathbb{E}[f(x) g(y)] &#x3D; \mathbb{E}[f(x)] \mathbb{E}[g(y)] \quad \text{if } x \perp y<br>$$</p>
<h3 id="Variance"><a href="#Variance" class="headerlink" title="Variance"></a>Variance</h3><ul>
<li><p>定義: 隨機變數 $X$ 的方差是其期望值的平方差的期望值</p>
<p>$$<br> \text{Var}(X) &#x3D; \mathbb{E}[(X - \mathbb{E}[X])^2] &#x3D; \mathbb{E}[X^2] - (\mathbb{E}[X])^2<br>$$</p>
</li>
<li><p>性質:</p>
</li>
</ul>
<p>$$<br>\text{Var}(aX + b) &#x3D; a^2 \text{Var}(X)<br>$$</p>
<h3 id="Covariance"><a href="#Covariance" class="headerlink" title="Covariance"></a>Covariance</h3><ul>
<li><p>定義: 隨機變數 $X$ 和 $Y$ 的協方差是它們偏離各自期望值的乘積的期望值</p>
<p>$$<br> \text{Cov}(X, Y) &#x3D; \mathbb{E}[(X - \mathbb{E}[X])(Y - \mathbb{E}[Y])] &#x3D; \mathbb{E}[XY] - \mathbb{E}[X] \mathbb{E}[Y]<br>$$</p>
</li>
</ul>
<p>可以用來衡量兩個隨機變數之間的線性關係，如果 $\text{Cov}(X, Y) &gt; 0$，表示 $X$ 和 $Y$ 傾向於同時增加或減少；如果 $\text{Cov}(X, Y) &lt; 0$，表示當 $X$ 增加時，$Y$ 傾向於減少，反之亦然</p>
<ul>
<li><p>如果 $X$ 和 $Y$ 是獨立的，則 $\text{Cov}(X, Y) &#x3D; 0$，但反之不一定成立</p>
</li>
<li><p>性質:</p>
</li>
</ul>
<p>$$<br>\text{Var}(aX + bY) &#x3D; a^2 \text{Var}(X) + b^2 \text{Var}(Y) + 2ab \text{Cov}(X, Y)<br>$$</p>
<p>$$<br>\text{Cov}(aX + b, cY + d) &#x3D; ac \text{Cov}(X, Y)<br>$$</p>
<p>$$<br>\text{Cov}(aX + bZ, cY + dW) &#x3D; ac \text{Cov}(X, Y) + ad \text{Cov}(X, W) + bc \text{Cov}(Z, Y) + bd \text{Cov}(Z, W)<br>$$</p>
<h3 id="Multivariate-Random-Variables"><a href="#Multivariate-Random-Variables" class="headerlink" title="Multivariate Random Variables"></a>Multivariate Random Variables</h3><p>$x &#x3D; [x_1, x_2, \ldots, x_n]^T$ 是一個 n 維隨機向量 (random vector)</p>
<ul>
<li><p>每個 $x_i$ 都是隨機變數，且彼此是 dependent 的</p>
</li>
<li><p>$P(x)$ 就是 $x_1, x_2, \ldots, x_n$ 的 joint distribution</p>
</li>
<li><p>Expectation Vector:</p>
<p>$$<br> \mathbb{E}[x] &#x3D; \begin{bmatrix}<br>  \mathbb{E}[x_1] \newline<br>  \mathbb{E}[x_2] \newline<br>  \vdots \newline<br>  \mathbb{E}[x_n]<br> \end{bmatrix} &#x3D; \begin{bmatrix}<br>  \mathbb{\mu}<em>{x_1} \newline<br>  \mathbb{\mu}</em>{x_2} \newline<br>  \vdots \newline<br>  \mathbb{\mu}_{x_n}<br> \end{bmatrix} &#x3D; \mu_x<br>$$</p>
</li>
<li><p>Covariance Matrix:</p>
<p>$$<br>  \begin{aligned}<br> \text{Cov}(x) &amp;&#x3D; \begin{bmatrix}<br>  \text{Var}(x_1) &amp; \text{Cov}(x_1, x_2) &amp; \ldots &amp; \text{Cov}(x_1, x_n) \newline<br>  \text{Cov}(x_2, x_1) &amp; \text{Var}(x_2) &amp; \ldots &amp; \text{Cov}(x_2, x_n) \newline<br>  \vdots &amp; \vdots &amp; \ddots &amp; \vdots \newline<br>  \text{Cov}(x_n, x_1) &amp; \text{Cov}(x_n, x_2) &amp; \ldots &amp; \text{Var}(x_n)<br> \end{bmatrix}  \newline<br> &amp;&#x3D; \Sigma_x \newline<br> &amp;&#x3D; \mathbb{E}[(x - \mu_x)(x - \mu_x)^T] \newline<br> &amp;&#x3D; \mathbb{E}[xx^T] - \mu_x \mu_x^T<br>  \end{aligned}<br>$$</p>
<ul>
<li>性質:<ul>
<li>$\Sigma_x$ 是 symmetric</li>
<li>$\Sigma_x$ 是 positive semi-definite</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Derived-Random-Variables"><a href="#Derived-Random-Variables" class="headerlink" title="Derived Random Variables"></a>Derived Random Variables</h3><p>令 $y &#x3D; f(x;w) &#x3D; w^T x$，其中 $y$ 是一個由隨機向量 $x$ 經過線性轉換得到的隨機變數，稱為 derived random variable</p>
<ul>
<li>性質<ul>
<li>$\mathbb{E}[y] &#x3D; w^T \mathbb{E}[x]$</li>
<li>$\text{Var}(y) &#x3D; w^T \text{Cov}(x) w &#x3D; w^T \Sigma_x w$</li>
<li>如果 $x$ 是 centralized 的 (mean zero)，則 $\mathbb{E}[y] &#x3D; 0$</li>
</ul>
</li>
</ul>
<h3 id="Bayes’-Rule"><a href="#Bayes’-Rule" class="headerlink" title="Bayes’ Rule"></a>Bayes’ Rule</h3><p>$$<br>P(A | B) &#x3D; \frac{P(B | A) P(A)}{P(B)} &#x3D; \frac{P(B | A) P(A)}{\sum_i P(B | A_i) P(A_i)}<br>$$</p>
<p>Bayes’ Rule 可以用來反轉條件機率，例如從 $P(B | A)$ 推斷 $P(A | B)$，每一項都有自己的名字：</p>
<p>$$<br>P(A | B): \text{Posterior} &#x3D; \frac{P(B | A): \text{Likelihood} \times P(A): \text{Prior}}{P(B): \text{Evidence}}<br>$$</p>
<h3 id="Point-Estimation"><a href="#Point-Estimation" class="headerlink" title="Point Estimation"></a>Point Estimation</h3><p>用樣本資料計算一個單一數值，用來估計未知的母體參數，例如：</p>
<ul>
<li><p>取樣本平均數作為母體平均數的估計</p>
<p>$$<br>\hat{\mu} &#x3D; \frac{1}{N} \sum_{i&#x3D;1}^{N} x_i<br>$$</p>
</li>
<li><p>取樣本比例作為母體比例的估計</p>
<p>$$<br>\hat{p} &#x3D; \frac{1}{N} \sum_{i&#x3D;1}^{N} I(x_i \in A)<br>$$</p>
</li>
<li><p>取樣本協方差作為母體協方差的估計</p>
<p>$$<br>\hat{\Sigma}<em>x &#x3D; \frac{1}{N} \sum</em>{i&#x3D;1}^{N} (x_i - \hat{\mu})(x_i - \hat{\mu})^T<br>$$</p>
<ul>
<li>如果 $x$ 是 centralized 的 (mean zero)，則 $\hat{\Sigma}<em>x &#x3D; \frac{1}{N} \sum</em>{i&#x3D;1}^{N} x_i x_i^T$</li>
</ul>
</li>
</ul>
<h3 id="Principal-Component-Analysis-PCA"><a href="#Principal-Component-Analysis-PCA" class="headerlink" title="Principal Component Analysis (PCA)"></a>Principal Component Analysis (PCA)</h3><h4 id="壓縮資訊"><a href="#壓縮資訊" class="headerlink" title="壓縮資訊"></a>壓縮資訊</h4><ul>
<li>給定資料集 $X &#x3D; {x_1, x_2, \ldots, x_N}$，每個 $x_i \in \mathbb{R}^D$</li>
<li>找到一個 function $f: \mathbb{R}^D \to \mathbb{R}^K$，其中 $K &lt; D$，使得 $f(x_i)$ 能夠保留 $x_i$ 的大部分資訊</li>
<li>目標是最小化重建誤差 (reconstruction error)，保留最大的資訊量</li>
</ul>
<h4 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h4><ul>
<li>假設 $x^{(i)}$ 為 randome variable $x$ 的 i.i.d. sample</li>
<li>假設 $f$ 是 Linear function，即 $f(x) &#x3D; W^T x$，其中 $W \in \mathbb{R}^{D \times K}$</li>
<li>PCA 的目標是找到 $K$ 個 orthonormal vectors $w_1, w_2, \ldots, w_K$ (principal components)，使得投影後的資料變異數 (variance) 最大化</li>
<li>為什麼 $w_1, w_2, \ldots, w_K$ 要是 orthonormal 的？<ul>
<li>避免 redundant information</li>
</ul>
</li>
<li>為什麼 $|w_i| &#x3D; 1$？<ul>
<li>避免 scale 的影響，不然可以無限放大 $w_i$ 來增加變異數的值</li>
</ul>
</li>
</ul>
<h4 id="Optimization-Problem"><a href="#Optimization-Problem" class="headerlink" title="Optimization Problem"></a>Optimization Problem</h4><p>為了簡化，先考慮 $K &#x3D; 1$ 的情況，我們要求最大的 $\text{Var}(z_1)$，其中 $z_1 &#x3D; w_1^T x$</p>
<p>而 $\text{Var}(z_1)$ 又可以寫成</p>
<p>$$<br>\text{Var}(z_1) &#x3D; \sigma_{z_1}^2 &#x3D; w_1^T \Sigma_x w_1<br>$$</p>
<p>如果先把 $x$ centralized (mean zero)，則</p>
<p>$$<br>\hat{\Sigma}<em>x &#x3D; \frac{1}{N} \sum</em>{i&#x3D;1}^{N} x_i x_i^T &#x3D; \frac{1}{N} X^TX<br>$$</p>
<p>接著，PCA 的目標可以寫成以下的優化問題：</p>
<p>$$<br>\begin{aligned}<br>\text{arg max}_{w_1 \in \mathbb{R}^D} \quad &amp; w_1^T X^T X w_1, \text{ subject to } |w_1|_2 &#x3D; 1 \newline<br>\end{aligned}<br>$$</p>
<p>又因為 $X^TX$ 是一個 real symmetric matrix，可以被 eigendecomposition 分解成</p>
<p>$$<br>X^TX &#x3D; W \Lambda W^T<br>$$</p>
<p>套用 Rayleigh’s Quotient 的結果，知道最大值會出現在最大的 eigen value 上，$w_1$ 就是其對應的 eigen vector</p>
<p>再來考慮 $w_2$，可以寫成以下的優化問題：</p>
<p>$$<br>\begin{aligned}<br>\text{arg max}_{w_2 \in \mathbb{R}^D} \quad &amp; w_2^T X^T X w_2, \newline<br>\text{ subject to } &amp; |w_2|_2 &#x3D; 1, &amp; w_2^T w_1 &#x3D; 0 \newline<br>\end{aligned}<br>$$</p>
<p>所以，對於一般情況， $w_1, w_2, \ldots, w_K$ 就是 $X^TX$ 前 $K$ 個最大的 eigen values 所對應的 eigen vectors</p>
<h3 id="Technical-Details"><a href="#Technical-Details" class="headerlink" title="Technical Details"></a>Technical Details</h3><h4 id="Sure-Almost-Sure-Events"><a href="#Sure-Almost-Sure-Events" class="headerlink" title="Sure &amp; Almost Sure Events"></a>Sure &amp; Almost Sure Events</h4><ul>
<li>Sure Event: 發生機率為 1 的事件，沒有任何例外情況<ul>
<li>擲一個公平的六面骰子，得到的點數一定在 1 到 6 之間</li>
</ul>
</li>
<li>Almost Sure Event: 發生機率為 1 的事件，但可能包含一些 measure zero 的例外情況<ul>
<li>在連續隨機變數中，取到某個特定值的機率為 0，但這並不代表該事件不可能發生</li>
</ul>
</li>
</ul>
<h4 id="Equality-of-Random-Variables"><a href="#Equality-of-Random-Variables" class="headerlink" title="Equality of Random Variables"></a>Equality of Random Variables</h4><p>感覺沒那麼重要，之後再補充</p>
<ul>
<li>Equality in Distribution</li>
<li>Almost Sure Equality</li>
<li>Equality</li>
</ul>
<h4 id="Convergence-of-Random-Variables"><a href="#Convergence-of-Random-Variables" class="headerlink" title="Convergence of Random Variables"></a>Convergence of Random Variables</h4><p>感覺沒那麼重要，之後再補充</p>
<ul>
<li>Convergence in Distribution</li>
<li>Convergence in Probability</li>
<li>Almost Sure Convergence</li>
</ul>
<h4 id="Distribution-of-Derived-Variables"><a href="#Distribution-of-Derived-Variables" class="headerlink" title="Distribution of Derived Variables"></a>Distribution of Derived Variables</h4><p>令 $y &#x3D; f(x)$ 且 $f^{-1}$ 存在，那 $P(y &#x3D; y) &#x3D; P(x &#x3D; f^{-1}(y))$ 一定成立嗎？</p>
<p>如果 $x$ 和 $y$ 都是連續的，<strong>不成立</strong></p>
<p>令 $x \sim \text{Uniform}(0, 1)$ 是連續的，且 $p(x) &#x3D; c$ for $x \in [0, 1]$</p>
<p>令 $y &#x3D; \frac{x}{2}$，則 $y \sim \text{Uniform}(0, 0.5)$</p>
<p>如果 $p_y(y) &#x3D; p_x(f^{-1}(y)) &#x3D; p_x(2y) &#x3D; c$，則</p>
<p>$$<br>\int_{0}^{0.5} p_y(y) dy &#x3D; \int_{0}^{0.5} c dy &#x3D; 0.5c &#x3D;  0.5 \ne 1<br>$$</p>
<p>不符合 PDF 的定義，所以不成立</p>
<h4 id="Jacobian-Adjustment"><a href="#Jacobian-Adjustment" class="headerlink" title="Jacobian Adjustment"></a>Jacobian Adjustment</h4><p>我們知道 $Pr(y &#x3D; y) &#x3D; p_y(y) dy$，且 $Pr(x &#x3D; x) &#x3D; p_x(x) dx$ 且</p>
<p>$$<br>|p_y(y) dy| &#x3D; |p_x(x) dx|<br>$$</p>
<p>可以得到</p>
<p>$$<br>p_y(y) &#x3D; p_x(f^{-1}(y)) \left| \frac{d f^{-1}(y)}{dy} \right|<br>$$</p>
<p>而在 multivariate 的情況下，Jacobian matrix $J_{f^{-1}}(y)$ 的行列式 (determinant) 就是我們要的調整因子</p>
<p>$$<br>p_y(y) &#x3D; p_x(f^{-1}(y)) \left| \det(J_{f^{-1}}(y)) \right|<br>$$</p>
<p>其中</p>
<p>$$<br>J_{f^{-1}}(y) &#x3D; \begin{bmatrix}<br>    \frac{\partial f_1^{-1}(y)}{\partial y_1} &amp; \frac{\partial f_1^{-1}(y)}{\partial y_2} &amp; \ldots &amp; \frac{\partial f_1^{-1}(y)}{\partial y_n} \newline<br>    \frac{\partial f_2^{-1}(y)}{\partial y_1} &amp; \frac{\partial f_2^{-1}(y)}{\partial y_2} &amp; \ldots &amp; \frac{\partial f_2^{-1}(y)}{\partial y_n} \newline<br>    \vdots &amp; \vdots &amp; \ddots &amp; \vdots \newline<br>    \frac{\partial f_n^{-1}(y)}{\partial y_1} &amp; \frac{\partial f_n^{-1}(y)}{\partial y_2} &amp; \ldots &amp; \frac{\partial f_n^{-1}(y)}{\partial y_n} \newline<br>\end{bmatrix}<br>$$</p>
<p>因為 function $f$ 在計算時可能會 distort 空間的體積，Jacobian determinant 就是用來調整這個體積變化的因子，在一維的情況下，就是導數的絕對值</p>
<h3 id="Probability-Distributions"><a href="#Probability-Distributions" class="headerlink" title="Probability Distributions"></a>Probability Distributions</h3><h4 id="Bernoulli-Distribution-Discrete"><a href="#Bernoulli-Distribution-Discrete" class="headerlink" title="Bernoulli Distribution (Discrete)"></a>Bernoulli Distribution (Discrete)</h4><p>給定一個參數 $p$，表示事件發生的機率，則 Bernoulli 分佈的 PMF 為</p>
<p>$$<br>P(X &#x3D; x) &#x3D; p^x (1-p)^{1-x}, \quad x \in {0, 1}<br>$$</p>
<ul>
<li><p>Expectation: $\mathbb{E}[X] &#x3D; p$</p>
<p>$$<br>\begin{aligned}<br>\mathbb{E}[X] &amp;&#x3D; \sum_{x \in {0, 1}} x P(X &#x3D; x) \newline<br>&amp;&#x3D; 0 \cdot P(X &#x3D; 0) + 1 \cdot P(X &#x3D; 1) \newline<br>&amp;&#x3D; 0 \cdot (1-p) + 1 \cdot p \newline<br>&amp;&#x3D; p \newline<br>\end{aligned}<br>$$</p>
</li>
<li><p>Variance: $\text{Var}(X) &#x3D; p(1-p)$</p>
<p>$$<br>\begin{aligned}<br>\text{Var}(X) &amp;&#x3D; \mathbb{E}[X^2] - (\mathbb{E}[X])^2 \newline<br>&amp;&#x3D; \sum_{x \in {0, 1}} x^2 P(X &#x3D; x) - p^2 \newline<br>&amp;&#x3D; 0^2 \cdot P(X &#x3D; 0) + 1^2 \cdot P(X &#x3D; 1) - p^2 \newline<br>&amp;&#x3D; 0^2 \cdot (1-p) + 1^2 \cdot p - p^2 \newline<br>&amp;&#x3D; p - p^2 \newline<br>&amp;&#x3D; p(1-p) \newline<br>\end{aligned}<br>$$</p>
</li>
</ul>
<h4 id="Categorical-Distribution-Discrete"><a href="#Categorical-Distribution-Discrete" class="headerlink" title="Categorical Distribution (Discrete)"></a>Categorical Distribution (Discrete)</h4><p>給定一個參數向量 $p &#x3D; [p_1, p_2, \ldots, p_K]$，表示 $K$ 個類別的機率，則 Categorical 分佈的 PMF 為</p>
<p>$$<br>P(X &#x3D; k) &#x3D; p_k, \quad k \in {1, 2, \ldots, K}<br>$$</p>
<p>or</p>
<p>$$<br>P(X &#x3D; k) &#x3D; \prod_{i&#x3D;1}^{K} p_i^{I(X &#x3D; i)}<br>$$</p>
<ul>
<li>$I(X &#x3D; i)$: indicator function，當 $X &#x3D; i$ 時為 1，否則為 0</li>
</ul>
<h4 id="Multinomial-Distribution-Discrete"><a href="#Multinomial-Distribution-Discrete" class="headerlink" title="Multinomial Distribution (Discrete)"></a>Multinomial Distribution (Discrete)</h4><p>給定一個參數向量 $p &#x3D; [p_1, p_2, \ldots, p_K]$，表示 $K$ 個類別的機率，且有 $n$ 次獨立試驗，則 Multinomial 分佈的 PMF 為</p>
<p>$$<br>P(X_1 &#x3D; x_1, X_2 &#x3D; x_2, \ldots, X_K &#x3D; x_K) &#x3D; \frac{n!}{x_1! x_2! \ldots x_K!} p_1^{x_1} p_2^{x_2} \ldots p_K^{x_K}<br>$$</p>
<ul>
<li>$x_i$: 第 $i$ 類別的次數，且 $\sum_{i&#x3D;1}^{K} x_i &#x3D; n$</li>
</ul>
<h4 id="Normal-Gaussian-Distribution-Continuous"><a href="#Normal-Gaussian-Distribution-Continuous" class="headerlink" title="Normal&#x2F;Gaussian Distribution (Continuous)"></a>Normal&#x2F;Gaussian Distribution (Continuous)</h4><p>給定一個參數 $\mu$ 和 $\sigma^2$，表示平均數和變異數，則 Normal 分佈的 PDF 為</p>
<p>$$<br>p(x) &#x3D; \frac{1}{\sqrt{2 \pi \sigma^2}} \exp \left( -\frac{(x - \mu)^2}{2 \sigma^2} \right)<br>$$</p>
<ul>
<li>Expectation: $\mathbb{E}[X] &#x3D; \mu$</li>
<li>Variance: $\text{Var}(X) &#x3D; \sigma^2$</li>
</ul>
<p>好處：</p>
<ul>
<li>提供最多的 Uncertainty (最大熵)</li>
<li>數學上很好處理 (continuous, differentiable)</li>
</ul>
<p>性質</p>
<ul>
<li><p>如果 $X \sim \mathcal{N}(\mu, \sigma^2)$，則 $Y &#x3D; aX + b \sim \mathcal{N}(a\mu + b, a^2 \sigma^2)$</p>
<p>$$<br>\begin{aligned}<br>\mathbb{E}[Y] &amp;&#x3D; \mathbb{E}[aX + b] &#x3D; a \mathbb{E}[X] + b &#x3D; a\mu + b \newline<br>\text{Var}(Y) &amp;&#x3D; \text{Var}(aX + b) &#x3D; a^2 \text{Var}(X) &#x3D; a^2 \sigma^2 \newline<br>\end{aligned}<br>$$</p>
<ul>
<li>所以做了 <code>z-normalization (standardization)</code> 後，$Z &#x3D; \frac{X - \mu}{\sigma} \sim \mathcal{N}(0, 1)$</li>
</ul>
</li>
<li><p>如果 $X_1 \sim \mathcal{N}(\mu_1, \sigma_1^2)$，$X_2 \sim \mathcal{N}(\mu_2, \sigma_2^2)$，且 $X_1$ 和 $X_2$ 獨立，則 $Y &#x3D; X_1 + X_2 \sim \mathcal{N}(\mu_1 + \mu_2, \sigma_1^2 + \sigma_2^2)$<br>$$<br>\begin{aligned}<br>\mathbb{E}[Y] &amp;&#x3D; \mathbb{E}[X_1 + X_2] &#x3D; \mathbb{E}[X_1] + \mathbb{E}[X_2] &#x3D; \mu_1 + \mu_2 \newline<br>\text{Var}(Y) &amp;&#x3D; \text{Var}(X_1 + X_2) &#x3D; \text{Var}(X_1) + \text{Var}(X_2) &#x3D; \sigma_1^2 + \sigma_2^2 \newline<br>\end{aligned}<br>$$</p>
</li>
</ul>
<h4 id="Multivariate-Normal-Gaussian-Distribution"><a href="#Multivariate-Normal-Gaussian-Distribution" class="headerlink" title="Multivariate Normal&#x2F;Gaussian Distribution"></a>Multivariate Normal&#x2F;Gaussian Distribution</h4><p>給定一個參數向量 $\mu \in \mathbb{R}^D$ 和協方差矩陣 $\Sigma \in \mathbb{R}^{D \times D}$，則 Multivariate Normal 分佈的 PDF 為</p>
<p>$$<br>p(x) &#x3D; \frac{1}{(2 \pi)^{D&#x2F;2} |\Sigma|^{1&#x2F;2}} \exp \left( -\frac{1}{2} (x - \mu)^T \Sigma^{-1} (x - \mu) \right)<br>$$</p>
<p>有空補</p>
<h3 id="Parametrizing-Functions"><a href="#Parametrizing-Functions" class="headerlink" title="Parametrizing Functions"></a>Parametrizing Functions</h3><p>用參數化的函數來描述資料的生成過程，例如：</p>
<ul>
<li>線性模型: $y &#x3D; w^T x + b$</li>
<li>決策樹: $y &#x3D; f(x; \theta)$，其中 $\theta$ 是樹的結構和分裂規則</li>
<li>神經網路: $y &#x3D; f(x; \Theta)$，其中 $\Theta$ 是網路的權重和偏差</li>
</ul>
<h4 id="Logistic-Function"><a href="#Logistic-Function" class="headerlink" title="Logistic Function"></a>Logistic Function</h4><p>$$<br>\sigma(z) &#x3D; \frac{1}{1 + \exp(-z)}<br>$$</p>
<ul>
<li>將實數映射到 (0, 1) 之間</li>
<li>常用於二元分類問題的輸出層</li>
</ul>
<h4 id="Softplus-Function"><a href="#Softplus-Function" class="headerlink" title="Softplus Function"></a>Softplus Function</h4><p>$$<br>\zeta(z) &#x3D; \log(1 + \exp(z))<br>$$</p>
<ul>
<li>soft 版本的 ReLU 函數</li>
</ul>
<h4 id="Softmax-Function"><a href="#Softmax-Function" class="headerlink" title="Softmax Function"></a>Softmax Function</h4><p>$$<br>\text{softmax}(z_i) &#x3D; \frac{\exp(z_i)}{\sum_{j} \exp(z_j)}<br>$$</p>
<ul>
<li>將實數向量映射到機率分佈</li>
<li>常用於多元分類問題的輸出層</li>
</ul>
<h3 id="Information-Theory"><a href="#Information-Theory" class="headerlink" title="Information Theory"></a>Information Theory</h3><p><strong>Probability</strong> 能夠在不確定的情況下，量化我們對事件發生的信心程度</p>
<p>而 <strong>Information Theory</strong> 則是量化不確定性本身，以及我們從觀察事件中獲得的資訊量</p>
<h4 id="Self-Information"><a href="#Self-Information" class="headerlink" title="Self-Information"></a>Self-Information</h4><p>事件 $x$ 發生所帶來的資訊量</p>
<p>$$<br> I(x) &#x3D; -\log P(x)<br>$$</p>
<ul>
<li>如果 $P(x)$ 越小，則 $I(x)$ 越大，表示罕見事件帶來更多資訊</li>
<li>如果 $P(x) &#x3D; 1$，則 $I(x) &#x3D; 0$，表示確定事件不帶來任何資訊</li>
</ul>
<h4 id="Entropy"><a href="#Entropy" class="headerlink" title="Entropy"></a>Entropy</h4><p>隨機變數 $X$ 的不確定性</p>
<h5 id="Shannon-Entropy"><a href="#Shannon-Entropy" class="headerlink" title="Shannon Entropy:"></a>Shannon Entropy:</h5><p>$$<br>H(X) &#x3D; -\sum_{x} P(x) \log P(x)<br>$$</p>
<h5 id="Differential-Entropy-Continuous"><a href="#Differential-Entropy-Continuous" class="headerlink" title="Differential Entropy (Continuous):"></a>Differential Entropy (Continuous):</h5><p>$$<br>H(X) &#x3D; -\int p(x) \log p(x) dx<br>$$</p>
<ul>
<li>Entropy 衡量一個隨機變數的不確定性或資訊量</li>
<li>Entropy 越大，表示不確定性越高</li>
<li>$0 \log 0$ 被定義為 $\text{lim}_{p \to 0^+} -p \log p &#x3D; 0$</li>
</ul>
<h4 id="Average-Code-Length"><a href="#Average-Code-Length" class="headerlink" title="Average Code Length"></a>Average Code Length</h4><p>Shannon Entropy 是最小平均編碼長度的下界，表示在最佳編碼方案下，平均每個符號所需的位元數</p>
<ul>
<li><p>Example:</p>
<p>假設有四個符號 $A, B, C, D$，其機率分別為 $P(A) &#x3D; 0.5, P(B) &#x3D; 0.25, P(C) &#x3D; 0.15, P(D) &#x3D; 0.1$</p>
<ul>
<li><p>Shannon Entropy 為</p>
<p>$$<br> H(X) &#x3D; -\sum_{x \in {A, B, C, D}} P(x) \log_2 P(x) &#x3D; 1.7427 \text{ bits}<br>$$</p>
</li>
<li><p>一種可能的編碼方案為:</p>
<ul>
<li>$A$: 0</li>
<li>$B$: 10</li>
<li>$C$: 110</li>
<li>$D$: 111</li>
</ul>
</li>
<li><p>平均編碼長度為</p>
<p>$$<br> L &#x3D; \sum_{x \in {A, B, C, D}} P(x) \cdot \text{length}(code(x)) &#x3D; 1.75 \text{ bits}<br>$$</p>
</li>
</ul>
<p>雖然平均編碼長度 $L &#x3D; 1.75$ bits 大於 Shannon Entropy $H(X) &#x3D; 1.7427$ bits，但已經非常接近，表示這是一個有效的編碼方案</p>
</li>
</ul>
<h4 id="Kullback-Leibler-KL-Divergence"><a href="#Kullback-Leibler-KL-Divergence" class="headerlink" title="Kullback-Leibler (KL) Divergence"></a>Kullback-Leibler (KL) Divergence</h4><p>用來衡量兩個機率分佈 $P$ 和 $Q$ 之間的差異</p>
<p>當 $P$ 是真實分佈，$Q$ 是近似分佈時，KL Divergence 衡量使用 $Q$ 來近似 $P$ 所帶來的資訊損失</p>
<p>$$<br>\begin{aligned}<br>D_{KL}(P || Q) &amp;&#x3D; \sum_{x} P(x) \log \frac{P(x)}{Q(x)} \newline<br>&amp;&#x3D; \mathbb{E}<em>{x \sim P} \left[ \log \frac{P(x)}{Q(x)} \right] \newline<br>&amp;&#x3D; \mathbb{E}</em>{x \sim P} [\log P(x) - \log Q(x)] \newline<br>&amp;&#x3D; -H(P) - \mathbb{E}_{x \sim P} [\log Q(x)] \newline<br>\end{aligned}<br>$$</p>
<p>其中 $\mathbb{E}_{x \sim P} [\log Q(x)]$ 是 <strong>cross-entropy</strong></p>
<p>如果 $P$ 和 $Q$ 是 Independent 的，$H(P)$ 不依賴於 $Q$，所以可以忽略，則</p>
<p>$$<br>\begin{aligned}<br>\arg\min_Q D_{KL}(P || Q) &amp;&#x3D; \arg\min_Q -\mathbb{E}<em>{x \sim P} [\log Q(x)] \newline<br>&amp;&#x3D; \arg\max_Q \mathbb{E}</em>{x \sim P} [\log Q(x)] \newline<br>\end{aligned}<br>$$</p>
<ul>
<li>性質<ul>
<li>$D_{KL}(P || Q) \geq 0$，且當且僅當 $P &#x3D; Q$ 時，等號成立</li>
<li>$D_{KL}(P || Q) \ne D_{KL}(Q || P)$，不對稱</li>
</ul>
</li>
</ul>
<h5 id="Minimizer-of-KL-Divergence"><a href="#Minimizer-of-KL-Divergence" class="headerlink" title="Minimizer of KL Divergence"></a>Minimizer of KL Divergence</h5><p>給定分布 $P$，尋找分布 $Q^*$ 使得 $D_{KL}(P || Q^*)$ 最小化</p>
<p>因為 KL Divergence 是不對稱的，我們有兩種方法：</p>
<ol>
<li>Forward KL Divergence: $\arg\min_Q D_{KL}(P || Q)$</li>
<li>Reverse KL Divergence: $\arg\min_Q D_{KL}(Q || P)$</li>
</ol>
<h3 id="Decision-Trees"><a href="#Decision-Trees" class="headerlink" title="Decision Trees"></a>Decision Trees</h3><p>Decision Trees 是一種監督式學習演算法，用於分類和迴歸問題</p>
<ul>
<li><p>Information Gain</p>
<p>$$<br>IG(D, A) &#x3D; H(D) - \sum_{v \in \text{Values}(A)} \frac{|D_v|}{|D|} H(D_v)<br>$$</p>
<ul>
<li>$D$: 資料集</li>
<li>$A$: 特徵</li>
<li>$D_v$: 在特徵 $A$ 上取值為 $v$ 的子集</li>
<li>$H(D)$: 資料集 $D$ 的熵</li>
<li>$\text{Values}(A)$: 特徵 $A$ 的所有可能取值</li>
</ul>
<p>選擇 Information Gain 最大的特徵來分裂節點，可以把 $H$ 看成是 Impurity 的量度，Impurity 越低，表示資料越純淨，$H$ 也越低</p>
</li>
</ul>
<ol>
<li>選擇 Information Gain 最大的特徵 $A^*$ 來分裂節點</li>
<li>對每個可能的取值 $v$，創建一個子節點 $D_v$</li>
<li>重複以上步驟，直到滿足停止條件 (例如，節點中的樣本數小於某個閾值，或所有樣本屬於同一類別)</li>
</ol>
<h3 id="Random-Forests"><a href="#Random-Forests" class="headerlink" title="Random Forests"></a>Random Forests</h3><p>Decision Tree 通常非常深，越深的節點越少經過訓練資料，容易 overfitting</p>
<p>Random Forests 是一種集成學習方法，通過結合多個 Decision Trees 來提高模型的泛化能力</p>
<ol>
<li>Bootstrap Aggregating (Bagging): 從原始資料集中有放回地抽取多個子集，對每個子集訓練一棵 Decision Tree</li>
<li>Feature Randomness: 在每個節點分裂時，隨機選擇 $k$ 個特徵來考慮，找出 Information Gain 最大的特徵來分裂節點</li>
<li>重複以上步驟，直到生成 $N$ 棵 Decision Trees</li>
<li>預測時，對所有 Decision Trees 的預測結果進行投票 (分類) 或平均 (迴歸)</li>
</ol>
<h2 id="Numerical-Optimization"><a href="#Numerical-Optimization" class="headerlink" title="Numerical Optimization"></a>Numerical Optimization</h2><h3 id="Numerical-Computation"><a href="#Numerical-Computation" class="headerlink" title="Numerical Computation"></a>Numerical Computation</h3><p>在機器學習中，會有大量的浮點數計算，受限於浮點數儲存的精度，有時候會產生 <strong>numeric errors</strong></p>
<h4 id="Overflow-Underflow"><a href="#Overflow-Underflow" class="headerlink" title="Overflow &amp; Underflow"></a>Overflow &amp; Underflow</h4><p>對於 Softmax 函數來說</p>
<p>$$<br>\text{softmax}(z_i) &#x3D; \frac{\exp(z_i)}{\sum_{j} \exp(z_j)}<br>$$</p>
<p>如果對於 $z_i &#x3D; c \ \forall i$，如果 $|c|$ 很大</p>
<ul>
<li>如果 $c$ 是正數，則 $\exp(c)$ 會 overflow</li>
<li>如果 $c$ 是負數，則 $\exp(c)$ 會 underflow，還有可能導致分母為 0</li>
</ul>
<p>為了解決這個問題，可以對 $z$ 進行平移 (shift)：</p>
<p>$$<br>\text{softmax}(z_i) &#x3D; \frac{\exp(z_i - z_{\max})}{\sum_{j} \exp(z_j - z_{\max})}<br>$$</p>
<p>這樣的計算結果會和原本的 softmax 函數相同，但可以避免 overflow 和 underflow 的問題</p>
<ul>
<li>分子最多是 $\exp(0) &#x3D; 1$，不會 overflow</li>
<li>分母至少有一項是 $\exp(0) &#x3D; 1$，不會為 0</li>
</ul>
<p>$$<br>\begin{aligned}<br>\text{softmax}(z_i) &amp;&#x3D; \frac{\exp(z_i - z_{\max})}{\sum_{j} \exp(z_j - z_{\max})} \newline<br>&amp;&#x3D; \frac{\exp(z_i) \exp(-z_{\max})}{\sum_{j} \exp(z_j) \exp(-z_{\max})} \newline<br>&amp;&#x3D; \frac{\exp(z_i)}{\sum_{j} \exp(z_j)} \newline<br>\end{aligned}<br>$$</p>
<h4 id="Poor-Conditioning"><a href="#Poor-Conditioning" class="headerlink" title="Poor Conditioning"></a>Poor Conditioning</h4><p><strong>Condition number</strong> 是用來衡量一個函數對輸入變化的敏感度</p>
<p>像是我們有 $f(x) &#x3D; Ax &#x3D; y$，其中 $A$ 是一個矩陣，且 $A^{-1}$ 存在</p>
<p>那麼其 condition number 定義為</p>
<p>$$<br>\kappa(A) &#x3D; \max_{i, j} \frac{|\lambda_i|}{|\lambda_j|}<br>$$</p>
<ul>
<li>$\lambda_i$: $A$ 的第 $i$ 個 eigen value，如前面所述，可以想成是對某個方向的伸縮最大值和最小值的比值</li>
<li>當 $\kappa(A)$ 很大時，對 $x$ 的微小變化會導致 $y$ 有很大的變化，這會影響到優化算法的收斂速度和穩定性，稱為 <strong>ill-conditioned</strong><ul>
<li>在解 $x &#x3D; A^{-1} y$ 時，會放大 $y$ 的 numeric errors，導致 $x$ 有很大的誤差</li>
</ul>
</li>
</ul>
<h3 id="Optimization-Problems"><a href="#Optimization-Problems" class="headerlink" title="Optimization Problems"></a>Optimization Problems</h3><p>Optimization problem 的目標是去最小化一個 <strong>cost function</strong> $f: \mathbb{R}^d \to \mathbb{R}$</p>
<p>$$<br>\text{argmin}_{x \in \mathbb{R}^d} f(x) \newline<br>\text{subject to } x \in \mathcal{C}<br>$$</p>
<p>$\mathcal{C} \subseteq \mathbb{R}^d$: constraint set，表示 $x$ 必須滿足的約束條件，又稱 為 feasible set，$x$ 稱為 feasible point</p>
<ul>
<li>例如: $\mathcal{C} &#x3D; {x : g_i(x) \leq 0, h_j(x) &#x3D; 0}$</li>
<li>如果 $\mathcal{C} &#x3D; \mathbb{R}^d$，稱為 unconstrained optimization problem</li>
<li>如果是最大化 objective function 問題，可以把目標改成最小化 $-f(x)$</li>
</ul>
<h4 id="Examples"><a href="#Examples" class="headerlink" title="Examples"></a>Examples</h4><ul>
<li>Critical Point: $\mathbf{C} &#x3D; {x : \nabla f(x) &#x3D; 0}$<ul>
<li>Minima: $\mathbf{C} &#x3D; {x : \nabla f(x) &#x3D; 0, H(f)(x) \succ 0}$</li>
<li>Maxima: $\mathbf{C} &#x3D; {x : \nabla f(x) &#x3D; 0, H(f)(x) \prec 0}$</li>
<li>Plateau&#x2F;Saddle Point: $\mathbf{C} &#x3D; {x : \nabla f(x) &#x3D; 0, H(f)(x) &#x3D; \mathbf{O} \ \text{or} \text{ indefinite}}$</li>
</ul>
</li>
<li>Global Minimum: $\min_{x \in \mathcal{C}} f(x) \in \mathbf{R}$</li>
<li>Optimal Point: $x^* \in \mathcal{C}$ such that $f(x^*) &#x3D; \min_{x \in \mathcal{C}} f(x)$</li>
</ul>
<h4 id="Convex-Optimization"><a href="#Convex-Optimization" class="headerlink" title="Convex Optimization"></a>Convex Optimization</h4><p>滿足以下條件的 optimization problem，稱為 convex optimization problem</p>
<ol>
<li>$H(f)(x) \succeq 0$ for all $x \in \mathcal{C}$</li>
<li>$g_i(x)$ 是 convex function for all $i$</li>
<li>$h_j(x)$ 是 affine function for all $j$<ul>
<li>affine function: $h(x) &#x3D; Ax + b$，就是 Linear + Constant shift</li>
</ul>
</li>
</ol>
<h3 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h3><p>根據 Taylor expansion，可以用函數 $\tilde{f}(x)$ 的多項式<br>近似來描述函數在某一點附近的行為，例如在點 $a$ 附近，可以用一階 Taylor expansion 來近似函數：</p>
<p>$$<br>f(x) \approx \tilde{f}(x) &#x3D; f(a) + \nabla f(a)^T (x - a)<br>$$</p>
<p>當我們選擇 $x &#x3D; a - \eta \nabla f(a)$ for some $\eta &gt; 0$ 時，可以得到</p>
<p>$$<br>\tilde{f}(x) &#x3D; f(a) - \eta |\nabla f(a)|^2 \leq \tilde{f}(a)<br>$$</p>
<h4 id="Negative-Gradient-is-the-Direction-of-Steepest-Descent"><a href="#Negative-Gradient-is-the-Direction-of-Steepest-Descent" class="headerlink" title="Negative Gradient is the Direction of Steepest Descent"></a>Negative Gradient is the Direction of Steepest Descent</h4><p>給定一個 function $f$，一個方向 $u$ 和點 $a$</p>
<p>Directional Derivative:</p>
<p>$$<br>D_u f(a) &#x3D; \lim_{h \to 0} \frac{f(a + h u) - f(a)}{h} &#x3D; \nabla f(a)^T u<br>$$</p>
<p>我們想找到一個單位向量 $u$，使得 $D_u f(a)$ 最小化</p>
<p>$$<br>\begin{aligned}<br>\arg\min_{u, |u| &#x3D; 1} D_u f(a) &amp;&#x3D; \arg\min_{u, |u| &#x3D; 1} \nabla f(a)^T u \newline<br>&amp;&#x3D; \arg\min_{u, |u| &#x3D; 1} |\nabla f(a)| |u| \cos \theta \newline<br>&amp;&#x3D; \arg\min_{u} \cos \theta \newline<br>\end{aligned}<br>$$</p>
<p>所以，當 $u &#x3D; -\frac{\nabla f(a)}{|\nabla f(a)|}$ 時，也就是負梯度方向，Directional Derivative 會達到最小值</p>
<h4 id="Set-Learning-Rate"><a href="#Set-Learning-Rate" class="headerlink" title="Set Learning Rate"></a>Set Learning Rate</h4><p>有空再補</p>
<h4 id="Problems-of-Gradient-Descent"><a href="#Problems-of-Gradient-Descent" class="headerlink" title="Problems of Gradient Descent"></a>Problems of Gradient Descent</h4><p>沒有考慮到 $H(f)(x)$ 的 Conditioning 的問題，可能在某個方向下降很快，但在另一個方向下降很慢，導致整體收斂速度變慢</p>
<ul>
<li>Zig-Zags</li>
</ul>
<h3 id="Newton’s-Method"><a href="#Newton’s-Method" class="headerlink" title="Newton’s Method"></a>Newton’s Method</h3><p>根據二維 Taylor expansion，可以用函數 $\tilde{f}(x)$ 的多項式近似來描述函數在某一點附近的行為，例如在點 $a$ 附近，可以用二階 Taylor expansion 來近似函數：</p>
<p>$$<br>f(x) \approx \tilde{f}(x) &#x3D; f(a) + \nabla f(a)^T (x - a) + \frac{1}{2} (x - a)^T H(f)(a) (x - a)<br>$$</p>
<p>當 $f$ 是 strictly convex 時，我們可以去解 $\nabla \tilde{f}(x) &#x3D; 0$，來找到 $\tilde{f}(x)$ 的最小值，得到</p>
<p>$$<br> a - H(f)(a)^{-1} \nabla f(a)<br>$$</p>
<ul>
<li>$H(f)(a)$ 就是 gradient 的 corrector</li>
</ul>
<p>在這種情況下，只要不斷找</p>
<p>$$<br>x^{(k+1)} &#x3D; x^{(k)} - \eta H(f)(x^{(k)})^{-1} \nabla f(x^{(k)})<br>$$</p>
<p>就可以快速收斂</p>
<p>當 $f$ 不是 strictly convex 時，$H(f)(x) \preceq 0$ 或 indefinite，會導致找出來的方向不是下降方向，因此會需要 <strong>Levenberg-Marquardt adjustment</strong></p>
<p>$$<br>x^{(k+1)} &#x3D; x^{(k)} - \eta (H(f)(x^{(k)}) + \lambda I)^{-1} \nabla f(x^{(k)})<br>$$</p>
<p>選一個足夠大的 $\lambda$，可以確保 $H(f)(x^{(k)}) + \lambda I \succ 0$，使得方向為下降方向</p>
<h4 id="Problems-of-Newton’s-Method"><a href="#Problems-of-Newton’s-Method" class="headerlink" title="Problems of Newton’s Method"></a>Problems of Newton’s Method</h4><ul>
<li>計算 $H(f)(x)$ 和 $H(f)(x)^{-1}$ 的時間和空間複雜度都很高，尤其是在高維度的情況下<ul>
<li>時間複雜度: $O(d^3)$</li>
<li>空間複雜度: $O(d^2)$</li>
</ul>
</li>
<li>$H(f)(x)$ 可能有很大的 Condition Number<ul>
<li>當 gradient 有 numeric errors 時，會被放大，導致方向不正確，越往後誤差越大</li>
</ul>
</li>
<li>可能會收斂到 Saddle Point<ul>
<li>因為在 Saddle Point 處，$H(f)(x)$ 是 indefinite，可能會導致找出來的方向不是下降方向</li>
</ul>
</li>
</ul>
<h3 id="Optimization-in-Machine-Learning"><a href="#Optimization-in-Machine-Learning" class="headerlink" title="Optimization in Machine Learning"></a>Optimization in Machine Learning</h3><p>大部分的機器學習模型都有 convex function，e.g. Linear Regression, Logistic Regression, SVM…</p>
<p>但是在深度學習中，通常都是 non-convex function，e.g. Neural Networks</p>
<h4 id="Lipschitz-Continuity"><a href="#Lipschitz-Continuity" class="headerlink" title="Lipschitz Continuity"></a>Lipschitz Continuity</h4><p>我們通常會假設 Cost Function $C$ 是 Lipschitz Continuous 的，表示存在一個常數 $K &gt; 0$，使得：</p>
<p>$$<br>|C(w^1) - C(w^2)| \leq K |w^1 - w^2|, \quad \forall w^1, w^2 \in \mathbb{R}^d<br>$$</p>
<p>這個條件保證了函數的變化不會太劇烈，有助於優化算法的收斂性分析</p>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/8/8d/Lipschitz_continuity.png" srcset="/notes/info/loading.gif" lazyload alt="Lipschitz Continuity"></p>
<h4 id="Perceptron-Learning-Algorithm"><a href="#Perceptron-Learning-Algorithm" class="headerlink" title="Perceptron Learning Algorithm"></a>Perceptron Learning Algorithm</h4><p>Binary Classification 問題</p>
<p>假設資料集 $D &#x3D; {(x_1, y_1), (x_2, y_2), \ldots, (x_N, y_N)}$，其中 $x_i \in \mathbb{R}^d$ 是特徵向量，$y_i \in {-1, 1}$ 是標籤</p>
<p>定義線性分類器 $f(x) &#x3D; sign(w^T x + b)$</p>
<ul>
<li>其中 $w \in \mathbb{R}^d$ 是權重向量，$b \in \mathbb{R}$ 是偏差</li>
<li>也可以把 $b$ 合併到 $w$ 中，令 $\tilde{x} &#x3D; [x; 1]$，$\tilde{w} &#x3D; [w; b]$，則 $f(x) &#x3D; \tilde{w}^T \tilde{x}$</li>
</ul>
<p>訓練時，每個 Epoch 會根據每筆資料 $(x^t, y^t)$ 來更新權重 $w$</p>
<p>$$<br>w^{(t+1)} &#x3D; w^{(t)} + \eta (y^t - \hat{y}^t) x^t<br>$$</p>
<ul>
<li><p>如果 $\hat{y}^t &#x3D; y^t$，則不更新</p>
</li>
<li><p>如果 $\hat{y}^t \ne y^t$，得到 $w^{(t+1)} &#x3D; w^{(t)} + 2 \eta y^t x^t$</p>
<ul>
<li>$y^t &#x3D; 1$<br>$$<br>\begin{aligned}<br>\text{sign}(w^{(t+1)T} x^t) &amp;&#x3D; \text{sign}((w^{(t)}  + 2 \eta x^t)^T x^t) \newline<br>&amp;&#x3D; \text{sign}(w^{(t)T} x^t + 2 \eta |x^t|^2) \newline<br>&amp;&#x3D; \text{sign}(w^{(t)T} x^t + c)<br>\end{aligned}<br>$$<br>因為 $c &gt; 0$，所以 $\text{sign}(w^{(t+1)T} x^t)$ 會傾向於變成 1</li>
</ul>
</li>
</ul>
<p>如果資料沒辦法線性可分，則 Perceptron Learning Algorithm 可能無法收斂</p>
<h4 id="ADAptive-LInear-NEuron-Adaline"><a href="#ADAptive-LInear-NEuron-Adaline" class="headerlink" title="ADAptive LInear NEuron (Adaline)"></a>ADAptive LInear NEuron (Adaline)</h4><p>Adaline 是一種線性分類器，與 Perceptron 類似，但使用連續的輸出值來進行學習</p>
<p>Cost Function:</p>
<p>$$<br>\arg\min_{w} \frac{1}{2} \sum_{i&#x3D;1}^{N} (y_i - w^T x_i)^2<br>$$</p>
<p>訓練完之後，使用 $f(x) &#x3D; sign(w^T x)$ 來進行分類</p>
<p>Update Rule:</p>
<p>$$<br>w^{(t+1)} &#x3D; w^{(t)} + \eta \sum_{i&#x3D;1}^{N} (y_i - w^{(t)T} x_i) x_i<br>$$</p>
<p>因為 Cost function 是 Convex 的，所以可以保證可以收斂到全域最小值</p>
<h4 id="Stochastic-Gradient-Descent-SGD"><a href="#Stochastic-Gradient-Descent-SGD" class="headerlink" title="Stochastic Gradient Descent (SGD)"></a>Stochastic Gradient Descent (SGD)</h4><p>把資料集 $D$ 分成多個 mini-batch，每次只使用一個 mini-batch 來更新權重</p>
<p>$$<br>w^{(t+1)} &#x3D; w^{(t)} - \eta \nabla C_{MB}(w^{(t)})<br>$$</p>
<ul>
<li>$C_{MB}(w)$: mini-batch 上的 Cost Function</li>
<li>每次更新只需要計算 mini-batch 的 gradient，計算量較小，適合大規模資料集</li>
<li>支援 Online Learning</li>
</ul>
<h3 id="Constrained-Optimization"><a href="#Constrained-Optimization" class="headerlink" title="Constrained Optimization"></a>Constrained Optimization</h3><p>Problem:</p>
<p>$$<br>\text{min}_{x} f(x) \newline<br>\text{subject to } x \in {x : g_i(x) \leq 0, h_j(x) &#x3D; 0}<br>$$</p>
<h4 id="Karush-Kuhn-Tucker-KKT-Methods"><a href="#Karush-Kuhn-Tucker-KKT-Methods" class="headerlink" title="Karush-Kuhn-Tucker (KKT) Methods"></a>Karush-Kuhn-Tucker (KKT) Methods</h4><p>可以把問題轉成</p>
<p>$$<br>\text{min}<em>x\text{max}</em>{\alpha, \beta, \alpha \geq 0} \mathcal{L}(x, \alpha, \beta) &#x3D; \newline<br>\text{min}<em>x\text{max}</em>{\alpha, \beta, \alpha \geq 0} f(x) + \sum_{i} \alpha_i g_i(x) + \sum_{j} \beta_j h_j(x) \newline<br>$$</p>
<p>當 $x$ 是 Feasible Point 時，$\mathcal{L}(x, \alpha, \beta) &#x3D; f(x)$，因為 $g_i(x) \leq 0$ 和 $h_j(x) &#x3D; 0$，會選 $\alpha_i &#x3D; 0$ 和任意 $\beta_j$ 來 maximize $\mathcal{L}(x, \alpha, \beta)$</p>
<p>當 $x$ 不是 Feasible Point 時，會有某些 $g_i(x) &gt; 0$ 或 $h_j(x) \ne 0$，這時候可以選擇很大的 $\alpha_i$ 或 $\beta_j$ 來讓 $\mathcal{L}(x, \alpha, \beta)$ 變無限大，這樣就不會選擇這些 $x$ 來 minimize $\mathcal{L}(x, \alpha, \beta)$</p>
<h4 id="KKT-Conditions"><a href="#KKT-Conditions" class="headerlink" title="KKT Conditions"></a>KKT Conditions</h4><p>假設 $f, g_i, h_j$ 都是可微分的，且 $x^*$ 是問題的最優解，則存在 $\alpha^*, \beta^*$，使得以下條件成立：</p>
<ol>
<li>Primal Feasibility: $g_i(x^*) \leq 0$ for all $i$, $h_j(x^*) &#x3D; 0$ for all $j$</li>
<li>Dual Feasibility: $\alpha_i^* \geq 0$ for all $i$</li>
<li>Stationarity: $\nabla f(x^*) + \sum_{i} \alpha_i^* \nabla g_i(x^*) + \sum_{j} \beta_j^* \nabla h_j(x^*) &#x3D; 0$</li>
<li>Complementary Slackness: $\alpha_i^* g_i(x^*) &#x3D; 0$ for all $i$</li>
</ol>
<h4 id="Complementary-Slackness"><a href="#Complementary-Slackness" class="headerlink" title="Complementary Slackness"></a>Complementary Slackness</h4><ul>
<li><p>如果 $g_i(x^*) &#x3D; 0$，稱為 active，$\alpha_i^* g_i(x^*) &#x3D; 0$</p>
</li>
<li><p>如果 $g_i(x^*) &lt; 0$，稱為 inactive，為了 maximize $\mathcal{L}(x^*, \alpha^*, \beta^*)$，必須有 $\alpha_i^* &#x3D; 0$</p>
</li>
</ul>
<p>今天我有一個 $\alpha_i^* &gt; 0$，我就可以知道 $g_i(x^*)$ 一定是 0，可以快速找到 active constraints，就可以確定最優解真正被哪些限制所決定</p>
<h3 id="The-Regression-Problem"><a href="#The-Regression-Problem" class="headerlink" title="The Regression Problem"></a>The Regression Problem</h3><p>給定資料集 $D &#x3D; {(x_1, y_1), (x_2, y_2), \ldots, (x_N, y_N)}$，其中 $x_i \in \mathbb{R}^d$ 是特徵向量，$y_i \in \mathbb{R}$ 是目標值</p>
<p>目標是找到一個函數 $f: \mathbb{R}^d \to \mathbb{R}$，使得對於所有的 $(x_i, y_i)$，$f(x_i)$ 盡可能接近 $y_i$</p>
<h4 id="Sum-of-Squared-Errors-SSE"><a href="#Sum-of-Squared-Errors-SSE" class="headerlink" title="Sum of Squared Errors (SSE):"></a>Sum of Squared Errors (SSE):</h4><p>$$<br>SSE(f; D) &#x3D; \sum_{i&#x3D;1}^{N} (y_i - f(x_i))^2<br>$$</p>
<h4 id="Mean-Squared-Error-MSE"><a href="#Mean-Squared-Error-MSE" class="headerlink" title="Mean Squared Error (MSE):"></a>Mean Squared Error (MSE):</h4><p>$$<br>MSE(f; D) &#x3D; \frac{1}{N} \sum_{i&#x3D;1}^{N} (y_i - f(x_i))^2<br>$$</p>
<h4 id="Relative-Squared-Error-RSE"><a href="#Relative-Squared-Error-RSE" class="headerlink" title="Relative Squared Error (RSE):"></a>Relative Squared Error (RSE):</h4><p>$$<br>RSE(f; D) &#x3D; \frac{\sum_{i&#x3D;1}^{N} (y_i - f(x_i))^2}{\sum_{i&#x3D;1}^{N} (y_i - \bar{y})^2}<br>$$</p>
<ul>
<li>$\bar{y} &#x3D; \frac{1}{N} \sum_{i&#x3D;1}^{N} y_i$: 目標值的平均值</li>
<li>一般來說，RSE 越小表示模型越好，如果算出來大於 1，表示模型比直接用平均值來預測還差，你乾脆直接用平均值算就好</li>
<li>R-squared ($R^2$) 是用來衡量模型解釋變異的比例<br>$$<br>R^2 &#x3D; 1 - RSE<br>$$<ul>
<li>$R^2$ 越接近 1，表示模型解釋變異的能力越強</li>
</ul>
</li>
</ul>
<h4 id="Data-Augmentation"><a href="#Data-Augmentation" class="headerlink" title="Data Augmentation"></a>Data Augmentation</h4><p>在機器學習中，可以把 Linear model 轉換成多項式特徵，或是加入其他變換，以增加模型的表達能力</p>
<p>[HW] How many variables to solve in $w$ for a polynomial regression problem with degree $P$</p>
<h3 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h3><p>為了 Generalization ，而不是只在訓練資料上表現好，可以在 Cost Function 中加入 Regularization term，來懲罰過於複雜的模型</p>
<p>$$<br>\text{argmin}_{w \in \mathbb{R}^d} \frac{1}{2} | y - (Xw - b)|^2 \newline<br>\text{subject to } |w|^2 \leq R<br>$$</p>
<p>可以寫成</p>
<p>$$<br>\text{argmin}_{w \in \mathbb{R}^{d+1}} \frac{1}{2} | y - (Xw )|^2 \newline<br>\text{subject to } w^T S w \leq R<br>$$</p>
<p>其中 $S &#x3D; \begin{bmatrix} 0 &amp; 0 \ 0 &amp; I_d \end{bmatrix}$</p>
<p>By KKT，可以轉成 unconstrained problem</p>
<p>$$<br>\text{argmin}<em>w \text{max}</em>{\alpha, \alpha \ge 0} \frac{1}{2} (| y - (Xw )|^2 + \alpha(  w^T S w - R))<br>$$</p>
<h3 id="Dual-Problem"><a href="#Dual-Problem" class="headerlink" title="Dual Problem"></a>Dual Problem</h3><p>給定 primal problem:</p>
<p>$$<br>p^* &#x3D; \text{min}<em>x \text{max}</em>{\alpha, \beta, \alpha \geq 0}  \mathcal{L}(x, \alpha, \beta)<br>$$</p>
<p>dual problem 定義為:</p>
<p>$$<br>d^* &#x3D; \text{max}_{\alpha, \beta, \alpha \geq 0} \text{min}_x  \mathcal{L}(x, \alpha, \beta)<br>$$</p>
<p>[HW] By max-min inequality, we have</p>
<p>$$<br>d^* \leq p^*<br>$$</p>
<p>當 primal problem 是 convex，且有解，則 strong duality 成立，$d^* &#x3D; p^*$，有時候解問題會更有效率</p>
<h4 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h4><p>考慮</p>
<p>$$<br>\text{argmin}_{w \in \mathbb{R}^d} \frac{1}{2} |x|^2 \newline<br>\text{subject to } Ax \geq b, A \in \mathbb{R}^{n \times d}, b \in \mathbb{R}^n<br>$$</p>
<p>By KKT，可以寫成</p>
<p>$$<br>\text{argmin}<em>x \text{max}</em>{\alpha, \alpha \geq 0} \frac{1}{2} |x|^2 + \alpha^T (b - Ax)<br>$$</p>
<p>因為是 Convex problem，滿足 Strong Duality，可以交換 min 和 max</p>
<p>$$<br>\text{argmax}_{\alpha, \alpha \geq 0} \text{min}_x \frac{1}{2} |x|^2 + \alpha^T (b - Ax)<br>$$</p>
<p>對 $x$ 求導並設為 0，得到 $x &#x3D; A^T \alpha$，帶回上式，得到</p>
<p>$$<br>\text{argmax}_{\alpha, \alpha \geq 0} -\frac{1}{2} |A^T \alpha|^2 + b^T \alpha<br>$$</p>
<p>現在只需要解 $n$ 維的問題，而不是 $d$ 維的問題，當 $n \ll d$ 時，會更有效率</p>
<h2 id="Learning-Theory"><a href="#Learning-Theory" class="headerlink" title="Learning Theory"></a>Learning Theory</h2><h4 id="Empirical-Error-risk"><a href="#Empirical-Error-risk" class="headerlink" title="Empirical Error &#x2F; risk"></a>Empirical Error &#x2F; risk</h4><p>Empirical Error 是在訓練資料集上計算的平均 Loss</p>
<p>$$<br>E_N[f] &#x3D; \frac{1}{N} \sum_{i&#x3D;1}^{N} \text{loss}(f(x_i; w), y_i)<br>$$</p>
<h4 id="Generalization-Error-risk"><a href="#Generalization-Error-risk" class="headerlink" title="Generalization Error &#x2F; risk"></a>Generalization Error &#x2F; risk</h4><p>Generalization Error 是在整個資料分佈上計算的平均 Loss，代表模型在未見過的資料上的表現</p>
<p>$$<br>C[f] &#x3D; \int \text{loss}(f(x; w), y) dP(x, y)<br>$$</p>
<p>Learining Theory 就是在理解怎麼去 Characterize</p>
<p>$$<br>C[f_N] &#x3D; \int \text{loss}(f_N(x; w), y) dP(x, y)<br>$$</p>
<ul>
<li>$f_N$: model learned from training data of size $N$</li>
<li>$C[f_N]$: generalization error</li>
</ul>
<h3 id="Bounding-Methods"><a href="#Bounding-Methods" class="headerlink" title="Bounding Methods"></a>Bounding Methods</h3><p>$min_fC[f] &#x3D; C[f^*]$ 稱為 Bayes Error，表示在給定資料分佈下，最佳模型的誤差下限</p>
<ul>
<li>如果 $P(y|x)$ 有隨機性，則 Bayes Error &gt; 0</li>
<li>目標是讓 $C[f_N]$ 越接近 $C[f^*]$ 越好</li>
</ul>
<p>$\Epsilon &#x3D; C(f_N) - C(f^*)$ 稱為 Excess Error，又可以寫成</p>
<p>$$<br>\Epsilon &#x3D; (C(f_F^*) - C(f^*)) + (C(f_N) - C(f_F^*))<br>$$</p>
<p>其中前項稱為 Approximation Error，後項稱為 Estimation Error</p>
<ul>
<li>選越複雜的 $F$，Approximation Error 越小，但 Estimation Error 越大</li>
</ul>
<h3 id="Properties-of-Estimators"><a href="#Properties-of-Estimators" class="headerlink" title="Properties of Estimators"></a>Properties of Estimators</h3><h4 id="Bias"><a href="#Bias" class="headerlink" title="Bias"></a>Bias</h4><ul>
<li>Bias 衡量估計值的期望與真實值之間的差異，來看這個 estimator 準不準</li>
<li>定義為<br>$$<br>\text{Bias}(\hat{\theta}) &#x3D; \mathbb{E}[\hat{\theta}] - \theta<br>$$</li>
<li>如果 $\text{Bias}(\hat{\theta}) &#x3D; 0$，則稱為無偏估計 (Unbiased Estimator)</li>
</ul>
<h4 id="Variance-1"><a href="#Variance-1" class="headerlink" title="Variance"></a>Variance</h4><ul>
<li>Variance 衡量估計值的變異程度，來看這個 estimator 穩不穩定</li>
<li>定義為<br>$$<br>\text{Var}(\hat{\theta}) &#x3D; \mathbb{E}[(\hat{\theta} - \mathbb{E}[\hat{\theta}])^2]<br>$$</li>
<li>Variance 越小，表示估計值越穩定</li>
</ul>
<h4 id="Sample-Mean-Estimator"><a href="#Sample-Mean-Estimator" class="headerlink" title="Sample Mean Estimator"></a>Sample Mean Estimator</h4><ul>
<li><p>給定資料集 $D &#x3D; {x_1, x_2, \ldots, x_N}$，樣本均值估計量定義為</p>
<p>$$<br>\hat{\mu} &#x3D; \frac{1}{N} \sum_{i&#x3D;1}^{N} x_i<br>$$</p>
</li>
<li><p>Bias:</p>
<p>$$<br>\begin{aligned}<br>\mathbb{E}[\hat{\mu}] &amp;&#x3D; \mathbb{E}\left[\frac{1}{N} \sum_{i&#x3D;1}^{N} x_i\right] \newline<br>&amp;&#x3D; \frac{1}{N} \sum_{i&#x3D;1}^{N} \mathbb{E}[x_i] \newline<br>&amp;&#x3D; \frac{1}{N} \sum_{i&#x3D;1}^{N} \mu \newline<br>&amp;&#x3D; \mu \newline<br>\end{aligned}<br>$$</p>
<ul>
<li>因此，$\text{Bias}(\hat{\mu}) &#x3D; \mathbb{E}[\hat{\mu}] - \mu &#x3D; 0$，樣本均值估計量是無偏的</li>
</ul>
</li>
<li><p>Variance:<br>$$<br>\begin{aligned}<br>\text{Var}<em>{\mathbb{X}}(\hat{\mu})<br>&amp;&#x3D; \mathbb{E}</em>{\mathbb{X}}[(\hat{\mu} - \mathbb{E}<em>{\mathbb{X}}[\hat{\mu}])^2] &#x3D; \mathbb{E}[\hat{\mu}^2 - 2\hat{\mu}\mu + \mu^2] &#x3D; \mathbb{E}[\hat{\mu}^2] - \mu^2 \<br>&amp;&#x3D; \mathbb{E}\left[\frac{1}{n^2} \sum</em>{i,j} x^{(i)}x^{(j)}\right] - \mu^2 &#x3D; \frac{1}{n^2} \sum_{i,j} \mathbb{E}[x^{(i)}x^{(j)}] - \mu^2 \<br>&amp;&#x3D; \frac{1}{n^2} \left( \sum_{i&#x3D;j} \mathbb{E}[x^{(i)}x^{(j)}] + \sum_{i \neq j} \mathbb{E}[x^{(i)}x^{(j)}] \right) - \mu^2 \<br>&amp;&#x3D; \frac{1}{n^2} \left( \sum_{i} \mathbb{E}[x^{(i)2}] + n(n-1)\mathbb{E}[x^{(i)}]\mathbb{E}[x^{(j)}] \right) - \mu^2 \<br>&amp;&#x3D; \frac{1}{n} \mathbb{E}[x^2] + \frac{(n-1)}{n} \mu^2 - \mu^2 &#x3D; \frac{1}{n} (\mathbb{E}[x^2] - \mu^2) &#x3D; \frac{1}{n} \sigma_{\text{x}}^2<br>\end{aligned}<br>$$</p>
<ul>
<li>因此，$\text{Var}(\hat{\mu}) &#x3D; \frac{1}{N} \sigma_{\text{x}}^2$，樣本均值估計量的變異程度隨著樣本數增加而減少</li>
</ul>
</li>
</ul>
<h4 id="Sample-Variance-Estimator"><a href="#Sample-Variance-Estimator" class="headerlink" title="Sample Variance Estimator"></a>Sample Variance Estimator</h4><ul>
<li><p>樣本變異數估計量定義為</p>
<p>$$<br>\hat{\sigma}^2 &#x3D; \frac{1}{N} \sum_{i&#x3D;1}^{N} (x_i - \hat{\mu})^2<br>$$</p>
</li>
<li><p>Bias:</p>
</li>
</ul>
<p>$$<br>\begin{aligned}<br>\text{E}_{\mathbb{X}}[\hat{\sigma}]<br>&amp;&#x3D; \text{E}\left[\frac{1}{n} \sum_i (x^{(i)} - \hat{\mu})^2 \right] &#x3D; \text{E}\left[ \frac{1}{n} (\sum_i x^{(i)2} - 2\sum_i x^{(i)}\hat{\mu} + \sum_i \hat{\mu}^2) \right] \<br>&amp;&#x3D; \text{E}\left[ \frac{1}{n} (\sum_i x^{(i)2} - n\hat{\mu}^2) \right] &#x3D; \frac{1}{n} \left( \sum_i \text{E}[x^{(i)2}] - n\text{E}[\hat{\mu}^2] \right) \<br>&amp;&#x3D; \text{E}[x^2] - \text{E}[\hat{\mu}^2] &#x3D; \text{E}[(x-\mu)^2 + 2x\mu - \mu^2] - \text{E}[\hat{\mu}^2] \<br>&amp;&#x3D; (\sigma^2 + \mu^2) - (\text{Var}[\hat{\mu}] + \text{E}[\hat{\mu}]^2) \<br>&amp;&#x3D; \sigma^2 + \mu^2 - \frac{1}{n}\sigma^2 - \mu^2 &#x3D; \frac{n-1}{n}\sigma^2 \neq \sigma^2<br>\end{aligned}<br>$$</p>
<ul>
<li><p>因此，$\text{Bias}(\hat{\sigma}^2) &#x3D; \mathbb{E}[\hat{\sigma}^2] - \sigma^2 &#x3D; -\frac{1}{N} \sigma^2$，樣本變異數估計量是有偏的</p>
</li>
<li><p>如果要 unbiased，可以改成</p>
<p>$$<br>\hat{\sigma}^2 &#x3D; \frac{1}{N-1} \sum_{i&#x3D;1}^{N} (x_i - \hat{\mu})^2<br>$$</p>
</li>
</ul>
<h4 id="Consistency"><a href="#Consistency" class="headerlink" title="Consistency"></a>Consistency</h4><ul>
<li>Consistency 衡量估計值隨著樣本數增加而收斂到真實值的能力</li>
<li>定義為<br>$$<br>\lim_{N \to \infty} P(|\hat{\theta}_N - \theta| &gt; \epsilon) &#x3D; 0, \quad \forall \epsilon &gt; 0<br>$$</li>
</ul>
<h3 id="Decomposing-Generalization-Error"><a href="#Decomposing-Generalization-Error" class="headerlink" title="Decomposing Generalization Error"></a>Decomposing Generalization Error</h3><p>前面提到，我們有一個模型 $f_N$，把它看作是一個 estimator，可以用上述方法分析這個 estimator 的 Bias 和 Variance</p>
<p>$$<br>\begin{aligned}<br>E_\mathbb{X}(C[f_N]) &amp;&#x3D; E_\mathbb{X}\left( \int (y - f_N(x))^2 P(x, y) dx dy \right) \newline<br>&amp;&#x3D; E_{\mathbb{X}, x, y}[\text{loss}(f_N(x) - y)] \newline<br>&amp;&#x3D; E_{x}(E_{\mathbb{X}, y}[\text{loss}(f_N(x) - y) | x &#x3D; x]) \newline<br>\end{aligned}<br>$$</p>
<p>在 Linear&#x2F; Polynomial Regression 的情況下</p>
<ul>
<li><p>$\text{loss}$ 是平方誤差</p>
</li>
<li><p>$y &#x3D; f^*(x) + \epsilon$，其中 $\epsilon$ 是高斯雜訊，$\epsilon \sim \mathcal{N}(0, \sigma^2)$</p>
<ul>
<li>$E_y[y|x] &#x3D; f^*(x)$</li>
<li>$\text{Var}_y[y|x] &#x3D; \sigma^2$</li>
</ul>
</li>
</ul>
<p>則</p>
<p>$$<br>\begin{aligned}<br>E_{\mathbb{X}, y}[\text{loss}(f_N(x) - y) | x] &amp;&#x3D; E_{\mathbb{X}, y}[(f_N(x) - y)^2 | x] \newline<br>&amp;&#x3D; E_{\mathbb{X}, y}[y^2 + f_N(x)^2 - 2y f_N(x) | x] \newline<br>&amp;&#x3D; E_{\mathbb{X}}[f_N(x)^2 | x] + E_y[y^2 | x] - 2 E_{\mathbb{X}, y}[yf_N(x) | x] \newline<br>&amp;&#x3D; (Var_\mathbb{X}[f_N(x) | x] + (E_{\mathbb{X}}[f_N(x) | x])^2) + (Var_y[y|x] + (E_y[y|x])^2) - 2 E_{\mathbb{X}}[f_N(x) | x] E_y[y|x] \newline<br>&amp;&#x3D; \text{Var}<em>y[y|x] + Var_\mathbb{X}[f_N(x) | x] + (E</em>{\mathbb{X}}[f_N(x) | x] - E_y[y|x])^2 \newline<br>&amp;&#x3D; \text{Var}<em>y[y|x] + Var_\mathbb{X}[f_N(x) | x] + (E</em>{\mathbb{X}}[f_N(x) - f^*(x) | x])^2 \newline<br>&amp;&#x3D; \sigma^2 + Var_\mathbb{X}[f_N(x) | x] + \text{Bias}[f_N(x) | x]^2 \newline<br>\end{aligned}<br>$$</p>
<p>得到</p>
<p>$$<br>\begin{aligned}<br>E_\mathbb{X}(C[f_N]) &amp;&#x3D; E_x\left( \sigma^2 + Var_\mathbb{X}[f_N(x) | x] + \text{Bias}[f_N(x) | x]^2 \right) \newline<br>\end{aligned}<br>$$</p>
<ul>
<li>$\sigma^2$: Irreducible Error，無法避免的誤差，因為資料本身有雜訊</li>
<li>$E_x(Var_\mathbb{X}[f_N(x) | x])$: Variance，模型對於不同訓練資料集的敏感度</li>
<li>$E_x(\text{Bias}[f_N(x) | x]^2)$: Bias，模型預測值與真實值的差異</li>
</ul>
<h2 id="Regularization-1"><a href="#Regularization-1" class="headerlink" title="Regularization"></a>Regularization</h2><p>前面有提到，為了避免 Overfitting，任何能夠讓 $f_N$ 更穩定的方式，都可以視為 Regularization</p>
<h3 id="Weight-Decay"><a href="#Weight-Decay" class="headerlink" title="Weight Decay"></a>Weight Decay</h3><h4 id="Occam’s-razor"><a href="#Occam’s-razor" class="headerlink" title="Occam’s razor"></a>Occam’s razor</h4><p>根據前面的 Learning Theory 的結果，兩個模型如果有一樣的 empirical error，應該選擇較簡單的模型，因為較簡單的模型通常有較低的 Variance，能夠更好地 Generalize 到未見過的資料</p>
<p>所以可以加上一個 Term 來去程罰複雜的模型</p>
<h4 id="Model-Complexity"><a href="#Model-Complexity" class="headerlink" title="Model Complexity"></a>Model Complexity</h4><ul>
<li>模型的 degree 不能直接拿來當作複雜度的衡量，因為它是固定的，不會隨著參數的改變而改變</li>
<li>可以用參數的範數 (norm) 來衡量模型的複雜度</li>
</ul>
<p>所以我們使用 $w$ 的 norm 作為正則化項，在 $w &#x3D; 0$ 時，模型最簡單</p>
<h4 id="Ridge-Regression-L2-Regularization"><a href="#Ridge-Regression-L2-Regularization" class="headerlink" title="Ridge Regression (L2 Regularization)"></a>Ridge Regression (L2 Regularization)</h4><p>$$<br>\text{argmin}_{w \in \mathbb{R}^d} \frac{1}{2} | y - (Xw - b)|^2 \text{subject to } |w|^2 \leq T<br>$$</p>
<p>一般會轉成 unconstrained problem</p>
<p>$$<br>\text{argmin}_{w \in \mathbb{R}^d} \frac{1}{2} | y - (Xw - b)|^2 + \frac{\alpha}{2} |w|^2<br>$$</p>
<ul>
<li>$\alpha$: regularization parameter，控制正則化項的權重，$\alpha$ 越大，模型越簡單</li>
<li>$b$ 通常不會被正則化，因為 $y$ 的平均值不一定是 0，所以 $b$ 不應該被懲罰</li>
</ul>
<h4 id="Lasso-Regression-L1-Regularization"><a href="#Lasso-Regression-L1-Regularization" class="headerlink" title="Lasso Regression (L1 Regularization)"></a>Lasso Regression (L1 Regularization)</h4><p>$$<br>\text{argmin}_{w \in \mathbb{R}^d} \frac{1}{2} | y - (Xw - b)|^2 + \alpha |w|_1<br>$$</p>
<ul>
<li>$|w|<em>1 &#x3D; \sum</em>{i&#x3D;1}^{d} |w_i|$</li>
<li>L1 regularization 會導致參數稀疏化，很多參數會被壓縮到 0，適合用於特徵選擇<ul>
<li>因為 L1 norm 的區域是菱形，會傾向於在坐標軸上取交點，把不重要的參數壓縮到 0</li>
</ul>
</li>
</ul>
<h5 id="Elastic-Net"><a href="#Elastic-Net" class="headerlink" title="Elastic Net"></a>Elastic Net</h5><p>LASSO 在某些情況下會有限制</p>
<ul>
<li>No Group Selection: 當有一組高度相關的特徵時，LASSO 可能只會選擇其中一個，而忽略其他相關特徵</li>
<li>最大選擇數量: LASSO 最多只能選擇 $N$ 個特徵，當特徵數量 $d$ 遠大於樣本數量 $N$ 時，LASSO 可能無法選擇足夠的特徵來建構模型</li>
</ul>
<p>Elastic Net 結合了 L1 和 L2 正則化的優點，可以同時達到稀疏化和穩定性的效果</p>
<p>$$<br>\text{argmin}_{w \in \mathbb{R}^d} \frac{1}{2} | y - (Xw - b)|^2 + \alpha(\beta |w|_1 + (1 - \beta) |w|^2)<br>$$</p>
<ul>
<li>$\beta \in [0, 1]$: 控制 L1 和 L2 正則化的權重比例，當 $\beta &#x3D; 1$ 時，等同於 LASSO；當 $\beta &#x3D; 0$ 時，等同於 Ridge Regression</li>
</ul>
<h3 id="Validation"><a href="#Validation" class="headerlink" title="Validation"></a>Validation</h3><ul>
<li>調整各種 hyperparameter (e.g. regularization parameter $\alpha$) 可以來控制模型的複雜度，以達到最佳的 Generalization Performance</li>
<li>通常會使用 Cross-Validation 來評估不同 hyperparameter 下模型的表現，選擇在驗證集上表現最好的參數組合</li>
</ul>
<h2 id="Maximum-Likelihood-Estimation-MLE"><a href="#Maximum-Likelihood-Estimation-MLE" class="headerlink" title="Maximum Likelihood Estimation (MLE)"></a>Maximum Likelihood Estimation (MLE)</h2><h3 id="Linear-Regression-via-MLE"><a href="#Linear-Regression-via-MLE" class="headerlink" title="Linear Regression via MLE"></a>Linear Regression via MLE</h3><p>假設 $y &#x3D; f^*(x) + \epsilon$，其中 $\epsilon$ 是高斯雜訊，$\epsilon \sim \mathcal{N}(0, \beta^{-1})$，則 $f^*(x)$ 可以寫成</p>
<p>$$<br>f^*(x; w^*) &#x3D; w^{*\top} x<br>$$</p>
<p>其中所有資料已經經過 z-normalization，平均值為 0，則</p>
<p>$$<br>(y | x) \sim \mathcal{N}(w^{*\top} x, \beta^{-1})<br>$$</p>
<p>所以目標會是找到一個 $w$ 接近 $w^*$</p>
<p>$$<br>\hat{y} &#x3D; \argmax_y P(y | x, w) &#x3D; w^{\top} x<br>$$</p>
<ul>
<li>$\beta^{-1}$ 跟 $w$ 無關，可以忽略</li>
</ul>
<p>MLE 的目標是最大化在所有資料點上的似然函數</p>
<p>$$<br>\text{argmax}_w P(\mathbf{X} | w)<br>$$</p>
<p>其中</p>
<p>$$<br>\begin{aligned}<br>P(\mathbf{X} | w) &amp;&#x3D; \prod_{i&#x3D;1}^{N} P(x_i, y_i | w) \newline<br>&amp;&#x3D; \prod_{i&#x3D;1}^{N} P(y_i | x_i, w) P(x_i | w) \newline<br>&amp;&#x3D; \prod_{i&#x3D;1}^{N} P(y_i | x_i, w) P(x_i) \newline<br>&amp;&#x3D; \prod_{i&#x3D;1}^{N} \mathcal{N}(y_i | w^{\top} x_i, \beta^{-1}) P(x_i) \newline<br>&amp;&#x3D; \prod_{i&#x3D;1}^{N}  \sqrt{\frac{\beta}{2\pi}} \exp\left( -\frac{\beta}{2} (y_i - w^{\top} x_i)^2 \right)  P(x_i) \newline<br>\end{aligned}<br>$$</p>
<p>先取 log，可以把乘法轉成加法，且 log 是單調遞增函數，不會影響最大值的位置</p>
<p>$$<br>\begin{aligned}<br>&amp;\text{argmax}<em>w \log P(\mathbf{X} | w) \newline<br>&amp;&#x3D; \text{argmax}<em>w \sum</em>{i&#x3D;1}^{N} \log  \prod</em>{i&#x3D;1}^{N} \left( \sqrt{\frac{\beta}{2\pi}} \exp\left( -\frac{\beta}{2} (y_i - w^{\top} x_i)^2 \right) \right) P(x_i)  \newline<br>&amp;&#x3D; \text{argmax}<em>w N \sqrt{\frac{\beta}{2\pi}} + \sum</em>{i&#x3D;1}^{N} P(x_i) - \frac{\beta}{2} \sum_{i&#x3D;1}^{N} (y_i - w^{\top} x_i)^2 \newline<br>\end{aligned}<br>$$</p>
<p>因為前兩項不依賴於 $w$，可以忽略，且 maximize 等同於 minimize 負的東西，所以可以寫成</p>
<p>$$<br>\text{argmin}<em>w \sum</em>{i&#x3D;1}^{N} (y_i - w^{\top} x_i)^2<br>$$</p>
<ul>
<li>這就是我們之前提到的 Sum of Squared Errors (SSE)</li>
<li>所以 MLE 在這個情況下等同於最小化 SSE，可以讓我們知道為什麼要最小化 SSE</li>
</ul>
<h3 id="Logistic-Regression-via-MLE"><a href="#Logistic-Regression-via-MLE" class="headerlink" title="Logistic Regression via MLE"></a>Logistic Regression via MLE</h3><p>假設 $y \in {0, 1}$，且<br>$$P(y&#x3D;1 | x, w) &#x3D; \sigma(w^{\top} x) &#x3D; \frac{1}{1 + \exp(-w^{\top} x)}$$</p>
<p>則</p>
<p>$$<br>P(y&#x3D;0 | x, w) &#x3D; 1 - \sigma(w^{\top} x) &#x3D; \frac{\exp(-w^{\top} x)}{1 + \exp(-w^{\top} x)}<br>$$</p>
<p>機率可以寫成</p>
<p>$$<br>P(y | x, w) &#x3D; \sigma(w^{\top} x)^y (1 - \sigma(w^{\top} x))^{1-y}<br>$$</p>
<p>MLE 的目標是最大化在所有資料點上的似然函數</p>
<p>$$<br>\text{argmax}_w P(y | x;w) &#x3D; \text{argmax}<em>w \prod</em>{i&#x3D;1}^{N} P(y_i | x_i, w)<br>$$</p>
<h2 id="Large-Scale-Machine-Learning"><a href="#Large-Scale-Machine-Learning" class="headerlink" title="Large-Scale Machine Learning"></a>Large-Scale Machine Learning</h2><p>Solve problems by leveraging the posteriror knowledge learned from the big data.</p>
<ul>
<li>Characteristics of Big Data:</li>
<li>Volume: 大量的資料</li>
<li>Variety: 多樣化的資料類型和來源</li>
<li>Velocity: 單位時間的資料量</li>
<li>Veracity: 資料的真實性和可靠性</li>
</ul>
<h2 id="Neural-Networks-Design"><a href="#Neural-Networks-Design" class="headerlink" title="Neural Networks: Design"></a>Neural Networks: Design</h2><ul>
<li>Feedforward neural networks (FNN) 又稱 multi-layer perceptron (MLP)</li>
</ul>
<p>$$</p>
<p>\begin{aligned}<br>\hat{y} &#x3D; f^{(L)}(\ldots f^{(2)}(f^{(1)}(x; \theta^{(1)}); \theta^{(2)}) \ldots; \theta^{(L)})<br>\end{aligned}</p>
<p>$$</p>
<ul>
<li><p>$L$: number of layers</p>
</li>
<li><p>$f^{(l)}$: nonlinear function of layer $l$</p>
</li>
<li><p>$\theta^{(l)}$: parameters of layer $l$</p>
</li>
<li><p>$x$: input vector</p>
</li>
<li><p>$f^{(k)}$ outputs value $a^{(k)}$, where</p>
</li>
</ul>
<p>$$</p>
<p>\begin{aligned}<br>a^{(k)} &amp;&#x3D; f^{(k)}(a^{(k-1)}; \theta^{(k)}) \newline<br>&amp;&#x3D; \text{act}^{(k)}(W^{(k)\top} a^{(k-1)} + b^{(k)}) \newline<br>\end{aligned}</p>
<p>$$</p>
<ul>
<li>$W^{(k)}$: weight matrix of layer $k$</li>
<li>$b^{(k)}$: bias vector of layer $k$</li>
<li>$\text{act}^{(k)}$: activation function of layer $k$</li>
<li>$a^{(0)} &#x3D; x$</li>
</ul>
<p>沒有非線性，模型就沒有深度，等價於單層線性模型而已。</p>
<h3 id="Training-an-NN"><a href="#Training-an-NN" class="headerlink" title="Training an NN"></a>Training an NN</h3><ul>
<li>Most NNs are trained using the <strong>maximum likelihood</strong> by default.</li>
</ul>
<p>$$</p>
<p>\begin{aligned}<br>\text{argmax}<em>{\Theta} \text{log } P(X | \Theta) &amp;&#x3D; \text{argmin}</em>{\Theta} -\text{log } P(X | \Theta) \newline<br>&amp;&#x3D; \text{argmin}<em>{\Theta} \Sigma_i -\text{log } P(x_i, y_i | \Theta) \newline<br>&amp;&#x3D; \text{argmin}</em>{\Theta} \Sigma<em>i [-\text{log } P(y_i | x_i, \Theta) - \text{log } P(x_i | \Theta)] \newline<br>&amp;&#x3D; \text{argmin}</em>{\Theta} \Sigma<em>i -\text{log } P(y_i | x_i, \Theta) \quad \text{(if we ignore } P(x_i | \Theta)) \newline<br>&amp;&#x3D; \text{argmin}</em>{\Theta} \Sigma_i C_i (\Theta)<br>\end{aligned}</p>
<p>$$</p>
<ul>
<li>通常 $P(x_i | \Theta)$ 不依賴於模型參數 $\Theta$，可以把他想像成常數，最小化時不會影響結果，所以可以忽略</li>
</ul>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/notes/categories/AI/" class="category-chain-item">AI</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/notes/tags/deep-learning/" class="print-no-link">#deep learning</a>
      
        <a href="/notes/tags/ai/" class="print-no-link">#ai</a>
      
        <a href="/notes/tags/machine-learning/" class="print-no-link">#machine learning</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>Deep Learning</div>
      <div>https://933yee.github.io/notes/2025/11/06/deep-learning/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>Author</div>
          <div>Kevin Lee</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>Posted on</div>
          <div>November 6, 2025</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>Licensed under</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - Attribution">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/notes/2025/11/13/paper/" title="Paper">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">Paper</span>
                        <span class="visible-mobile">Previous</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/notes/2025/11/02/leetcode-contest/" title="LeetCode Contest">
                        <span class="hidden-mobile">LeetCode Contest</span>
                        <span class="visible-mobile">Next</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>Table of Contents</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">Keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/notes/js/events.js" ></script>
<script  src="/notes/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/notes/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script  src="/notes/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/notes/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">Blog works best with JavaScript enabled</div>
  </noscript>
</body>
</html>
