

<!DOCTYPE html>
<html lang="en" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/notes/info/fluid.png">
  <link rel="icon" href="/notes/info/avatar.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Kevin Lee">
  <meta name="keywords" content="">
  
    <meta name="description" content="Quantized Side Tuning: Fast and Memory-Efficient Tuning of Quantized Large Language Models摘要本文提出一種名為 Quantized Side Tuning (QST) 的訓練框架，旨在快速且記憶體效率高地微調大型語言模型（LLMs）。該方法透過兩階段處理：  將模型權重量化為 4-bit，減少儲存所需的記憶體">
<meta property="og:type" content="article">
<meta property="og:title" content="Paper">
<meta property="og:url" content="https://933yee.github.io/notes/2025/11/13/paper/index.html">
<meta property="og:site_name" content="933yee&#39;s Notes">
<meta property="og:description" content="Quantized Side Tuning: Fast and Memory-Efficient Tuning of Quantized Large Language Models摘要本文提出一種名為 Quantized Side Tuning (QST) 的訓練框架，旨在快速且記憶體效率高地微調大型語言模型（LLMs）。該方法透過兩階段處理：  將模型權重量化為 4-bit，減少儲存所需的記憶體">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2025-11-13T08:36:37.000Z">
<meta property="article:modified_time" content="2025-11-13T11:17:43.167Z">
<meta property="article:author" content="Kevin Lee">
<meta property="article:tag" content="research">
<meta property="article:tag" content="paper">
<meta name="twitter:card" content="summary_large_image">
  
  
  
  <title>Paper - 933yee&#39;s Notes</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/notes/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/notes/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/notes/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"933yee.github.io","root":"/notes/","version":"1.9.7","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/info/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/notes/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/notes/js/utils.js" ></script>
  <script  src="/notes/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 7.1.1"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/notes/">
      <strong>933yee&#39;s Notes</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/notes/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>Home</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/notes/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>Archives</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/notes/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>Categories</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/notes/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>Tags</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/notes/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>About</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/notes/info/default.gif') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.7)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="Paper"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-11-13 16:36" pubdate>
          November 13, 2025 pm
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          3.6k words
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          30 mins
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">Paper</h1>
            
            
              <div class="markdown-body">
                
                <h2 id="Quantized-Side-Tuning-Fast-and-Memory-Efficient-Tuning-of-Quantized-Large-Language-Models"><a href="#Quantized-Side-Tuning-Fast-and-Memory-Efficient-Tuning-of-Quantized-Large-Language-Models" class="headerlink" title="Quantized Side Tuning: Fast and Memory-Efficient Tuning of Quantized Large Language Models"></a>Quantized Side Tuning: Fast and Memory-Efficient Tuning of Quantized Large Language Models</h2><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>本文提出一種名為 Quantized Side Tuning (QST) 的訓練框架，旨在快速且記憶體效率高地微調大型語言模型（LLMs）。該方法透過兩階段處理：</p>
<ol>
<li>將模型權重量化為 4-bit，減少儲存所需的記憶體。</li>
<li>設計一個與主模型分離的「側邊網路（side network）」，該網路僅根據主模型的隱藏層輸出進行任務預測，從而避免在主模型上進行反向傳播，進一步減少中介激活值的記憶體消耗。</li>
</ol>
<p>此外，QST 運用低秩適配器（如 LoRA、Adapter）與無梯度下取樣模組來降低可訓練參數數量。實驗顯示，QST 能夠將總記憶體占用降低至最多 1&#x2F;7，訓練速度提升 3 倍，並在準確率上保持與現有最先進方法相當的水準。</p>
<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>隨著大型語言模型（LLMs）不斷增大（達數百億參數），其在微調過程中所需的記憶體與計算資源急遽上升。現有的 PEFT（參數高效率微調）雖能部分減少訓練參數，但仍需保留大量中介激活值，因此無法從根本上解決記憶體瓶頸。本文提出 QST，目的在於同時解決模型權重、優化器狀態與中介激活值三大記憶體來源問題。</p>
<h3 id="研究目的本研究主要目的為："><a href="#研究目的本研究主要目的為：" class="headerlink" title="研究目的本研究主要目的為："></a>研究目的本研究主要目的為：</h3><ul>
<li>設計一個針對量化 LLMs 的記憶體節省且快速微調方法。</li>
<li>減少三個主要記憶體負擔來源：模型權重、優化器狀態與中介激活值。</li>
<li>維持微調後模型的高效性能，並提升在多種下游任務上的表現。</li>
</ul>
<h3 id="文獻探討"><a href="#文獻探討" class="headerlink" title="文獻探討"></a>文獻探討</h3><ul>
<li><p>2.1 Parameter-Efficient Finetuning：<br>回顧 PEFT 方法如 LoRA、Adapters、Prompt Tuning 等，其特點在於僅微調少量參數；但這些方法仍需保留大部分中介激活值，導致記憶體需求仍偏高。</p>
</li>
<li><p>2.2 Memory-Efficient Training and Finetuning：<br>探討如 Gradient Checkpointing、可逆網路、網路剪枝與蒸餾等降低記憶體需求技術，指出這些技術或需額外運算、或不適用於大模型；因此提出 QST 為一種可針對全模型大小皆適用的解法。</p>
</li>
</ul>
<h3 id="研究方法"><a href="#研究方法" class="headerlink" title="研究方法"></a>研究方法</h3><ul>
<li><p>研究設計：<br>提出 QST 架構，包含 4-bit 量化與 side tuning 雙階段設計。</p>
</li>
<li><p>研究對象：<br>使用多種主流 LLM 架構（如 OPT、LLaMA-2）進行實驗，涵蓋模型參數從 1.3B 至 70B。</p>
</li>
<li><p>研究工具：<br>實作於 PyTorch 與 HuggingFace 平台，使用 NF4 為 4-bit 資料型態、bfloat16 為計算型態。</p>
</li>
<li><p>資料處理與分析：<br>評估模型在 GLUE、MMLU 等標準資料集上的準確率與記憶體消耗，並以 FLOPS&#x2F;token 量測訓練速度。</p>
</li>
</ul>
<h3 id="研究結果"><a href="#研究結果" class="headerlink" title="研究結果"></a>研究結果</h3><ul>
<li>記憶體消耗： QST 可比 QLoRA 降低記憶體使用量高達 2.3 倍。</li>
<li>訓練速度： 相較 QLoRA 提升 3 倍以上。</li>
<li>準確率： 在 GLUE 與 MMLU 數據集上與 QLoRA 等方法表現相當，誤差小於 1%。</li>
<li>模型擴展性： 可有效應用於 70B 參數的 LLaMA-2 模型。</li>
<li>下取樣模組比較： Adapter 為最佳方案，在效能與記憶體效率間取得平衡。</li>
<li>應用測試： 在 MT-Bench 聊天測試中，QST 甚至超越原始 LLaMA-2-70B 模型。</li>
</ul>
<h3 id="結論與建議"><a href="#結論與建議" class="headerlink" title="結論與建議"></a>結論與建議</h3><ul>
<li>結論：<ul>
<li>QST 為一項具突破性的微調方法，能在不犧牲效能的情況下，顯著降低訓練資源消耗，特別適合應用於記憶體受限的現實場景。</li>
</ul>
</li>
<li>建議：<ul>
<li>未來可擴展 QST 至多模態模型。</li>
<li>探索更多無參數下取樣方法，以進一步降低資源需求。</li>
<li>深入研究 QST 於推論時的優化策略。</li>
</ul>
</li>
</ul>
<h3 id="需要查的"><a href="#需要查的" class="headerlink" title="需要查的"></a>需要查的</h3><ul>
<li>gradient checkpointing</li>
<li>PEFT</li>
<li>quantization</li>
<li>QLoRA</li>
<li>finetuning</li>
<li>side network</li>
<li>LLM head</li>
</ul>
<h2 id="ADO-LLM-Analog-Design-Bayesian-Optimization-with-In-Context-Learning-of-Large-Language-Models"><a href="#ADO-LLM-Analog-Design-Bayesian-Optimization-with-In-Context-Learning-of-Large-Language-Models" class="headerlink" title="ADO-LLM: Analog Design Bayesian Optimization with In-Context Learning of Large Language Models"></a>ADO-LLM: Analog Design Bayesian Optimization with In-Context Learning of Large Language Models</h2><ul>
<li><p>BO: Bayesian Optimization</p>
<p>想像一下你正在一個伸手不見五指的漆黑房間裡，你想找到房間中最深的那個「坑」（也就是最佳設計點）。</p>
<p>唯一能做的，就是用一根很長的拐杖去戳地上的某個點，然後測量那個點的深度。</p>
<ul>
<li><p><strong>問題</strong>：房間太大了，你不可能把每個點都戳一遍。而且每戳一次（代表一次電路模擬）都要花費很多時間和力氣 。</p>
</li>
<li><p><strong>目標</strong>：用最少的次數，找到那個最深的坑。</p>
</li>
</ul>
<p>這時，BO (貝氏優化) 就是你採用的「聰明策略」：</p>
<ol>
<li><p>**開始 (戳幾個點)**：你先隨便戳 5 個點，然後把它們的深度和位置記在一張紙上 。</p>
</li>
<li><p>**畫出「猜想地圖」 (代理模型)**：你看著這 5 個點的數據，開始在腦中「猜想」整個房間的地形。你會想：「嗯，A 點和 B 點都很深，它們中間的 C 點可能也很深。」這張你猜想的地圖，就是 BO 中的「代理模型」(Surrogate Model) 。</p>
</li>
<li><p>**決定下一步戳哪裡 (採集函數)**：這時你面臨一個抉擇：</p>
<ul>
<li><p>**利用 (Exploitation)**：你應該在你目前找到的「最深點」附近再多戳幾下，因為真正的最深點很可能就在旁邊 。</p>
</li>
<li><p>**探索 (Exploration)**：或者，你應該去一個你「完全沒戳過」的遙遠角落戳一下？雖然那裡可能很淺，但也萬一那裡藏著一個更深的坑呢？</p>
</li>
</ul>
</li>
<li><p><strong>聰明的 BO</strong>：BO 的厲害之處在於，它有一個數學公式（叫做「採集函數」），可以幫你完美地平衡這兩種衝動。它會告訴你「下一步戳哪裡」的 CP 值最高。</p>
</li>
<li><p><strong>重複</strong>：你戳了 BO 建議的那個新點，更新了你腦中的地圖 ，然後 BO 再給你下一個建議……如此循環，直到你找到那個最深的坑。</p>
</li>
</ol>
<p>BO 是一種用最少嘗試次數，去找到「黑盒子」（你不知道內部運作原理，只能靠嘗試）的最佳答案的優化方法。</p>
<p>在這篇論文中：</p>
<ul>
<li>房間就是「所有可能的電路參數組合」。</li>
<li>坑的深度就是「電路設計的好壞」(FOM) 。</li>
<li>戳一次拐杖就是「跑一次 HSPICE 電路模擬」。</li>
<li>BO 的局限：如果房間太大（高維度問題），BO 還是會覺得很貴很慢 。</li>
<li>ADO-LLM 的幫助：這就像你旁邊多了一個「有經驗的專家」(LLM)，他雖然也看不見，但他會根據經驗跟你說：「我猜西邊角落可能不錯。」 這能讓 BO 更快地找到高價值的區域。</li>
</ul>
</li>
</ul>
<table>
<thead>
<tr>
<th>特性</th>
<th>BO (貝氏優化)</th>
<th>LLM (大型語言模型)</th>
</tr>
</thead>
<tbody><tr>
<td>本質</td>
<td>一個數學優化工具</td>
<td>一個知識生成模型</td>
</tr>
<tr>
<td>知識來源</td>
<td>零 。它從零開始，只學習「當下」這批模擬數據 。</td>
<td>龐大。它從海量的預訓練數據（網路、書籍）中學到了先驗知識 。</td>
</tr>
<tr>
<td>如何工作</td>
<td>依賴數學（採集函數）來平衡「探索」和「利用」 。</td>
<td>依賴語言、範例（In-Context Learning）和推理（Chain-of-Thought） 。</td>
</tr>
<tr>
<td>優點</td>
<td>擅長探索。能系統性地搜索整個 空間，不會被範例限制 。</td>
<td>擅長利用知識。能快速生成「可行」的設計點 ，理解電路原理 。</td>
</tr>
<tr>
<td>缺點</td>
<td>黑盒子 。它不懂「類比電路」，只看數字 。在高維度時效率低 。</td>
<td>不擅長探索。傾向於「模仿」看過的範例 ，容易卡在局部最佳解 。</td>
</tr>
</tbody></table>
<h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>這篇論文的核心是提出一個名為 ADO-LLM 的新框架，它首次將大型語言模型 (LLM) 與貝氏優化 (Bayesian Optimization, BO) 結合起來，用於自動化類比電路設計 。</p>
<ul>
<li><p><strong>問題背景</strong>：類比電路設計非常依賴人類的專業知識，是提高生產效率的瓶頸 。</p>
</li>
<li><p>**現有方法 (BO)**：貝氏優化 (BO) 是一種流行的機器學習策略，已被用於自動化類比設計 。但傳統 BO 在高維度問題上，無論是計算成本還是數據使用效率都很高。</p>
</li>
<li><p><strong>ADO-LLM 的解決方案</strong>：</p>
<ul>
<li><p><strong>LLM 輔助 BO</strong>：ADO-LLM 利用 LLM 注入領域知識的能力，快速生成可行的設計點，以彌補 BO 在數據稀少時尋找高價值設計區域的低效率 。</p>
</li>
<li><p><strong>BO 輔助 LLM</strong>：同時，BO 迭代過程中評估的設計點，為 LLM 提供了高質量的「範例」(demonstrations)，使其能生成更高質量的設計 。</p>
</li>
<li><p><strong>互利共生</strong>：BO 的「探索」能力（Exploration）帶來了設計的多樣性，這豐富了 LLM 的上下文理解，使其能更廣泛地搜索，並避免提出重複的建議 。</p>
</li>
</ul>
</li>
<li><p><strong>結論</strong>：該框架在兩種不同的類比電路上進行了評估，證實其在設計效率和成效上都有顯著改進。</p>
</li>
</ul>
<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>引言深入探討了「為什麼」需要這個新框架。</p>
<ul>
<li><p><strong>類比電路設計的挑戰</strong>：</p>
<ul>
<li><strong>設計空間巨大</strong>：類比電路「尺寸設定」(sizing) 是一個極其複雜的任務 。</li>
<li><strong>多目標權衡</strong>：設計師需要在多個競爭目標（如功率、面積、性能）之間找到微妙的平衡 。</li>
</ul>
</li>
<li><p><strong>現有方法的局限</strong>：</p>
<ul>
<li><p><strong>BO 的局限</strong>：</p>
<ol>
<li><strong>黑盒子</strong>：BO 缺乏特定領域的類比設計知識 。它只看最終的「品質因數」(FOM)，而不懂電晶體操作區域等模擬器提供的「過程反饋」。</li>
<li><strong>多目標困難</strong>：BO 擅長單目標優化，但在多目標權衡上表現不佳 。</li>
</ol>
</li>
<li><p><strong>LLM 的局限</strong>：</p>
<ol>
<li><strong>依賴範例</strong>：LLM 的優化質量高度依賴輸入範例 (demonstrations) 的質量 。</li>
<li><strong>不敢探索</strong>：LLM 傾向於「模仿」它所看到的範例，不願意探索範例之外的新設計區域 。</li>
</ol>
</li>
</ul>
</li>
<li><p><strong>ADO-LLM 的動機</strong>： 本文提出的 ADO-LLM 框架 ，旨在結合兩者的優點：</p>
<ul>
<li>BO 受益於 LLM 的領域知識，能更快找到好的設計區域 。</li>
<li>LLM 受益於 BO 的探索能力，能看到更多樣化、高質量的範例，從而打破模仿的限制，更廣泛地搜索 。</li>
</ul>
</li>
</ul>
<h3 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h3><p>這部分解釋了 ADO-LLM 的具體運作方式，您可以對照圖 1 來理解。</p>
<p>該框架主要由四個組件構成：GP-BO 提議器、LLM 代理、高質量數據採樣器和 HSPICE 模擬器 。</p>
<p><strong>步驟 1：初始化 (左側 1, 2 區塊)</strong></p>
<ol>
<li><p><strong>設定 LLM</strong>：一開始，研究人員會向 LLM 代理提供電路定義、設計規格（例如：增益 &gt; 60dB）和設計指令 。</p>
</li>
<li><p><strong>零樣本 (Zero-shot) 生成</strong>：LLM 代理利用其龐大的預訓練知識，以「零樣本」的方式（即不看任何範例）生成一組初始的設計點 。</p>
</li>
</ol>
<p><strong>步驟 2：優化迴圈 (右側 3 區塊)</strong> 這是框架運作的核心。LLM 和 BO 會並行工作，提出新的設計參數。</p>
<ol>
<li><p>GP-BO 提議器 (BO 的工作)：</p>
<ul>
<li>BO 會查看所有「已收集數據」。</li>
<li>它使用高斯過程 (GP) 模型來預測整個設計空間 。</li>
<li>它的目標是平衡「探索」（Exploration，嘗試不確定的新區域）和「利用」（Exploitation，深入已知的高分區域）。</li>
<li>BO 會提出 4 個新的候選參數（根據實驗設定）。</li>
</ul>
</li>
<li><p>LLM 代理 (LLM 的工作)：</p>
<ul>
<li>高質量數據採樣器：首先，採樣器會從「已收集數據」中，挑選出 FOM (品質因數) 排名前 k（例如前 5 名）的高質量範例 。</li>
<li>情境學習 (In-context Learning)：LLM 會讀取這些「少樣本範例」(Few-shot Demonstrations) 。</li>
<li>思考鏈 (Chain-of-Thought)：LLM (使用 ChatGPT-3.5 Turbo ) 會被引導進行多步推理 ，例如：a. 解釋電路定義 → b. 權衡設計規格 → c. 參考少樣本範例 → d. 遵循設計原則（例如確保電晶體在特定區域工作）。</li>
<li>LLM 會提出 1 個新的候選參數 。</li>
</ul>
</li>
<li><p>模擬與迭代：</p>
<ul>
<li>來自 BO (4 個) 和 LLM (1 個) 的所有新參數，都會被送入 HSPICE 模擬器進行評估 。</li>
<li>模擬結果（包含 FOM 和電晶體狀態等）會作為「新數據」被添加回「已收集數據」中 。</li>
<li>重複步驟 2，迴圈繼續。</li>
</ul>
</li>
</ol>
<h3 id="Experiments-and-Results"><a href="#Experiments-and-Results" class="headerlink" title="Experiments and Results"></a>Experiments and Results</h3><p>研究人員在兩個電路上測試了 ADO-LLM，並與單獨的「GP-BO」（隨機初始化）和「LLM 代理」進行了比較 。</p>
<p><strong>實驗 1：兩級差動放大器 (Table 3)</strong></p>
<ul>
<li>結果：ADO-LLM 是唯一滿足所有 5 項設計規格（0 個未達標）的方法 。</li>
<li>效率：ADO-LLM 達到了最高的 FOM (3.52) 。相比之下，GP-BO 即使多花 4 倍的模擬次數 (80 次迭代)，FOM 也只有 2.10，且仍有 1 個規格未達標 。</li>
<li>結論：這證明了 ADO-LLM 在類比電路設計上的數據效率 (data efficiency) 。</li>
</ul>
<p><strong>實驗 2：遲滯比較器 (Table 5)</strong></p>
<ul>
<li>結果：結果一致。ADO-LLM 再次成為唯一滿足所有規格的方法（0 個未達標），並取得了最高的 FOM (0.90) 。</li>
<li>結論：GP-BO 和 LLM 代理單獨使用時，雖然可能在某些指標上表現出色，但無法在有限的模擬預算內平衡所有指標 。</li>
</ul>
<p><strong>消融研究 (Ablation Studies)</strong></p>
<ul>
<li><p>**LLM 零樣本初始化的重要性 (Table 6)**：</p>
<ul>
<li>研究顯示，使用 LLM 提供的初始點（GP-BO with LLM’s init）比使用隨機初始點（GP-BO with random init）能獲得更好的 FOM 。</li>
<li>這證實了 LLM 的先驗知識確實能為優化提供一個更好的起點 。</li>
</ul>
</li>
<li><p>**高質量範例的重要性 (Table 7)**：</p>
<ul>
<li>研究比較了三種 LLM 代理：a. 不看範例 (w&#x2F;o ICL)、b. 看隨機 5 個範例 (Rand 5)、c. 看 Top 5 範例。</li>
<li>結果顯示，使用「Top 5」高質量範例的 LLM 代理，在 FOM 和滿足規格數量上均表現最佳 。</li>
<li>有趣的是，如果給 LLM 看「隨機」範例，在比較器電路上甚至比不看範例（零樣本）的表現更差 ，這證明了範例的「質量」至關重要。</li>
</ul>
</li>
</ul>
<h3 id="Limitations-and-Future-Work"><a href="#Limitations-and-Future-Work" class="headerlink" title="Limitations and Future Work"></a>Limitations and Future Work</h3><p>儘管 ADO-LLM 表現出色，但仍有其局限性：</p>
<ol>
<li><p>依賴外部 LLM API：目前依賴 ChatGPT 這種閉源 API，成本高且缺乏透明度 。且這些通用 LLM 並非專為電路設計定製 。</p>
<ul>
<li>未來：可以開發針對 EDA 任務的領域適應 LLM ，或使用 RAG 技術來整合更精確的領域知識 。</li>
</ul>
</li>
<li><p>BO 的可擴展性：高斯過程 (GP) 在面對更複雜、設計空間更龐大的電路時，可能難以擴展 。</p>
<ul>
<li>未來：可以實施分層優化策略，模仿真實人類專家的做法，每次只優化一部分參數 。</li>
</ul>
</li>
</ol>
<h3 id="能問的問題"><a href="#能問的問題" class="headerlink" title="能問的問題"></a>能問的問題</h3><ol>
<li>只用其中一種方法（例如只用 LLM）來做電路設計，它最大的問題會是什麼？</li>
<li>他的 LLM 是用什麼模型？為什麼選這個？</li>
<li>我可以理解成 BO 和 LLM 的合作方式是把他們的輸出放到一個共享的資料庫?</li>
<li>FOM 是什麼</li>
<li>BO 是甚麼</li>
<li>為什麼 Zero-Shot Initialization</li>
</ol>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/notes/categories/paper/" class="category-chain-item">paper</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/notes/tags/research/" class="print-no-link">#research</a>
      
        <a href="/notes/tags/paper/" class="print-no-link">#paper</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>Paper</div>
      <div>https://933yee.github.io/notes/2025/11/13/paper/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>Author</div>
          <div>Kevin Lee</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>Posted on</div>
          <div>November 13, 2025</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>Licensed under</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - Attribution">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/notes/2025/11/06/deep-learning/" title="Deep Learning">
                        <span class="hidden-mobile">Deep Learning</span>
                        <span class="visible-mobile">Next</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>Table of Contents</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">Keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/notes/js/events.js" ></script>
<script  src="/notes/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/notes/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/notes/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/notes/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">Blog works best with JavaScript enabled</div>
  </noscript>
</body>
</html>
